{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Review Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the exercise\n",
    "\n",
    "For this exercise, you'll be working in groups to build a model to classify emails as spam based on a variety of feaures which have been extracted from the raw text of those emails. \n",
    "\n",
    "The methods and techniques for solving the questions below can be found throughout the lecture notes, however, you should also apply your own knowledge, intuition, and creativity to build the best possible model. The performance of your model will measured by the classification accuracy on the test data.\n",
    "\n",
    "### About the data\n",
    " The dataset was built by researchers at the UCI Machine Learning Institute: https://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "*From the UCI dataset documentation*:\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... \n",
    "\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "The last column of 'spambase.data' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail. The run-length attributes (55-57) measure the length of sequences of consecutive capital letters. \n",
    "\n",
    "### Attributes: \n",
    "_______________-\n",
    "\n",
    "####48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail. A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string. \n",
    "\n",
    "####6 continuous real [0,100] attributes of type char_freq_CHAR] \n",
    "= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail \n",
    "\n",
    "####1 continuous real [1,...] attribute of type capital_run_length_average \n",
    "= average length of uninterrupted sequences of capital letters \n",
    "\n",
    "####1 continuous integer [1,...] attribute of type capital_run_length_longest \n",
    "= length of longest uninterrupted sequence of capital letters \n",
    "\n",
    "####1 continuous integer [1,...] attribute of type capital_run_length_total \n",
    "= sum of length of uninterrupted sequences of capital letters \n",
    "= total number of capital letters in the e-mail \n",
    "\n",
    "####1 nominal {0,1} class attribute of type spam \n",
    "= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. \n",
    "\n",
    "\n",
    "## Relevant Documentation\n",
    "\n",
    "#### Relevant Classifiers\n",
    "- **Logistic Regression** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- **K Neighbors Classifier** http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "#### Preprocessing and Feature Selection\n",
    "- **Select K Best** http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "- **f_classif** http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\n",
    "\n",
    "#### Model Evaluation\n",
    "- **ROC Curve** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "- **cross validation** http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',60)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_people</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_addresses</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_business</th>\n",
       "      <th>word_freq_email</th>\n",
       "      <th>word_freq_you</th>\n",
       "      <th>word_freq_credit</th>\n",
       "      <th>word_freq_your</th>\n",
       "      <th>word_freq_font</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>word_freq_money</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_hpl</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_650</th>\n",
       "      <th>word_freq_lab</th>\n",
       "      <th>word_freq_labs</th>\n",
       "      <th>word_freq_telnet</th>\n",
       "      <th>word_freq_857</th>\n",
       "      <th>word_freq_data</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_85</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>word_freq_1999</th>\n",
       "      <th>word_freq_parts</th>\n",
       "      <th>word_freq_pm</th>\n",
       "      <th>word_freq_direct</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_meeting</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_project</th>\n",
       "      <th>word_freq_re</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_table</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "      <td>  4601.000000</td>\n",
       "      <td> 4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>    0.104553</td>\n",
       "      <td>    0.213015</td>\n",
       "      <td>    0.280656</td>\n",
       "      <td>    0.065425</td>\n",
       "      <td>    0.312223</td>\n",
       "      <td>    0.095901</td>\n",
       "      <td>    0.114208</td>\n",
       "      <td>    0.105295</td>\n",
       "      <td>    0.090067</td>\n",
       "      <td>    0.239413</td>\n",
       "      <td>    0.059824</td>\n",
       "      <td>    0.541702</td>\n",
       "      <td>    0.093930</td>\n",
       "      <td>    0.058626</td>\n",
       "      <td>    0.049205</td>\n",
       "      <td>    0.248848</td>\n",
       "      <td>    0.142586</td>\n",
       "      <td>    0.184745</td>\n",
       "      <td>    1.662100</td>\n",
       "      <td>    0.085577</td>\n",
       "      <td>    0.809761</td>\n",
       "      <td>    0.121202</td>\n",
       "      <td>    0.101645</td>\n",
       "      <td>    0.094269</td>\n",
       "      <td>    0.549504</td>\n",
       "      <td>    0.265384</td>\n",
       "      <td>    0.767305</td>\n",
       "      <td>    0.124845</td>\n",
       "      <td>    0.098915</td>\n",
       "      <td>    0.102852</td>\n",
       "      <td>    0.064753</td>\n",
       "      <td>    0.047048</td>\n",
       "      <td>    0.097229</td>\n",
       "      <td>    0.047835</td>\n",
       "      <td>    0.105412</td>\n",
       "      <td>    0.097477</td>\n",
       "      <td>    0.136953</td>\n",
       "      <td>    0.013201</td>\n",
       "      <td>    0.078629</td>\n",
       "      <td>    0.064834</td>\n",
       "      <td>    0.043667</td>\n",
       "      <td>    0.132339</td>\n",
       "      <td>    0.046099</td>\n",
       "      <td>    0.079196</td>\n",
       "      <td>    0.301224</td>\n",
       "      <td>    0.179824</td>\n",
       "      <td>    0.005444</td>\n",
       "      <td>    0.031869</td>\n",
       "      <td>    0.038575</td>\n",
       "      <td>    0.139030</td>\n",
       "      <td>    0.016976</td>\n",
       "      <td>    0.269071</td>\n",
       "      <td>    0.075811</td>\n",
       "      <td>    0.044238</td>\n",
       "      <td>    5.191515</td>\n",
       "      <td>   52.172789</td>\n",
       "      <td>   283.289285</td>\n",
       "      <td>    0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>    0.305358</td>\n",
       "      <td>    1.290575</td>\n",
       "      <td>    0.504143</td>\n",
       "      <td>    1.395151</td>\n",
       "      <td>    0.672513</td>\n",
       "      <td>    0.273824</td>\n",
       "      <td>    0.391441</td>\n",
       "      <td>    0.401071</td>\n",
       "      <td>    0.278616</td>\n",
       "      <td>    0.644755</td>\n",
       "      <td>    0.201545</td>\n",
       "      <td>    0.861698</td>\n",
       "      <td>    0.301036</td>\n",
       "      <td>    0.335184</td>\n",
       "      <td>    0.258843</td>\n",
       "      <td>    0.825792</td>\n",
       "      <td>    0.444055</td>\n",
       "      <td>    0.531122</td>\n",
       "      <td>    1.775481</td>\n",
       "      <td>    0.509767</td>\n",
       "      <td>    1.200810</td>\n",
       "      <td>    1.025756</td>\n",
       "      <td>    0.350286</td>\n",
       "      <td>    0.442636</td>\n",
       "      <td>    1.671349</td>\n",
       "      <td>    0.886955</td>\n",
       "      <td>    3.367292</td>\n",
       "      <td>    0.538576</td>\n",
       "      <td>    0.593327</td>\n",
       "      <td>    0.456682</td>\n",
       "      <td>    0.403393</td>\n",
       "      <td>    0.328559</td>\n",
       "      <td>    0.555907</td>\n",
       "      <td>    0.329445</td>\n",
       "      <td>    0.532260</td>\n",
       "      <td>    0.402623</td>\n",
       "      <td>    0.423451</td>\n",
       "      <td>    0.220651</td>\n",
       "      <td>    0.434672</td>\n",
       "      <td>    0.349916</td>\n",
       "      <td>    0.361205</td>\n",
       "      <td>    0.766819</td>\n",
       "      <td>    0.223812</td>\n",
       "      <td>    0.621976</td>\n",
       "      <td>    1.011687</td>\n",
       "      <td>    0.911119</td>\n",
       "      <td>    0.076274</td>\n",
       "      <td>    0.285735</td>\n",
       "      <td>    0.243471</td>\n",
       "      <td>    0.270355</td>\n",
       "      <td>    0.109394</td>\n",
       "      <td>    0.815672</td>\n",
       "      <td>    0.245882</td>\n",
       "      <td>    0.429342</td>\n",
       "      <td>   31.729449</td>\n",
       "      <td>  194.891310</td>\n",
       "      <td>   606.347851</td>\n",
       "      <td>    0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    1.000000</td>\n",
       "      <td>    1.000000</td>\n",
       "      <td>     1.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    1.588000</td>\n",
       "      <td>    6.000000</td>\n",
       "      <td>    35.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.100000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    1.310000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.220000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.065000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    2.276000</td>\n",
       "      <td>   15.000000</td>\n",
       "      <td>    95.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.420000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.380000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.160000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.800000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.100000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    2.640000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    1.270000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.110000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.188000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    0.315000</td>\n",
       "      <td>    0.052000</td>\n",
       "      <td>    0.000000</td>\n",
       "      <td>    3.706000</td>\n",
       "      <td>   43.000000</td>\n",
       "      <td>   266.000000</td>\n",
       "      <td>    1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>    4.540000</td>\n",
       "      <td>   14.280000</td>\n",
       "      <td>    5.100000</td>\n",
       "      <td>   42.810000</td>\n",
       "      <td>   10.000000</td>\n",
       "      <td>    5.880000</td>\n",
       "      <td>    7.270000</td>\n",
       "      <td>   11.110000</td>\n",
       "      <td>    5.260000</td>\n",
       "      <td>   18.180000</td>\n",
       "      <td>    2.610000</td>\n",
       "      <td>    9.670000</td>\n",
       "      <td>    5.550000</td>\n",
       "      <td>   10.000000</td>\n",
       "      <td>    4.410000</td>\n",
       "      <td>   20.000000</td>\n",
       "      <td>    7.140000</td>\n",
       "      <td>    9.090000</td>\n",
       "      <td>   18.750000</td>\n",
       "      <td>   18.180000</td>\n",
       "      <td>   11.110000</td>\n",
       "      <td>   17.100000</td>\n",
       "      <td>    5.450000</td>\n",
       "      <td>   12.500000</td>\n",
       "      <td>   20.830000</td>\n",
       "      <td>   16.660000</td>\n",
       "      <td>   33.330000</td>\n",
       "      <td>    9.090000</td>\n",
       "      <td>   14.280000</td>\n",
       "      <td>    5.880000</td>\n",
       "      <td>   12.500000</td>\n",
       "      <td>    4.760000</td>\n",
       "      <td>   18.180000</td>\n",
       "      <td>    4.760000</td>\n",
       "      <td>   20.000000</td>\n",
       "      <td>    7.690000</td>\n",
       "      <td>    6.890000</td>\n",
       "      <td>    8.330000</td>\n",
       "      <td>   11.110000</td>\n",
       "      <td>    4.760000</td>\n",
       "      <td>    7.140000</td>\n",
       "      <td>   14.280000</td>\n",
       "      <td>    3.570000</td>\n",
       "      <td>   20.000000</td>\n",
       "      <td>   21.420000</td>\n",
       "      <td>   22.050000</td>\n",
       "      <td>    2.170000</td>\n",
       "      <td>   10.000000</td>\n",
       "      <td>    4.385000</td>\n",
       "      <td>    9.752000</td>\n",
       "      <td>    4.081000</td>\n",
       "      <td>   32.478000</td>\n",
       "      <td>    6.003000</td>\n",
       "      <td>   19.829000</td>\n",
       "      <td> 1102.500000</td>\n",
       "      <td> 9989.000000</td>\n",
       "      <td> 15841.000000</td>\n",
       "      <td>    1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  word_freq_receive  word_freq_will  \\\n",
       "count      4601.000000     4601.000000        4601.000000     4601.000000   \n",
       "mean          0.090067        0.239413           0.059824        0.541702   \n",
       "std           0.278616        0.644755           0.201545        0.861698   \n",
       "min           0.000000        0.000000           0.000000        0.000000   \n",
       "25%           0.000000        0.000000           0.000000        0.000000   \n",
       "50%           0.000000        0.000000           0.000000        0.100000   \n",
       "75%           0.000000        0.160000           0.000000        0.800000   \n",
       "max           5.260000       18.180000           2.610000        9.670000   \n",
       "\n",
       "       word_freq_people  word_freq_report  word_freq_addresses  \\\n",
       "count       4601.000000       4601.000000          4601.000000   \n",
       "mean           0.093930          0.058626             0.049205   \n",
       "std            0.301036          0.335184             0.258843   \n",
       "min            0.000000          0.000000             0.000000   \n",
       "25%            0.000000          0.000000             0.000000   \n",
       "50%            0.000000          0.000000             0.000000   \n",
       "75%            0.000000          0.000000             0.000000   \n",
       "max            5.550000         10.000000             4.410000   \n",
       "\n",
       "       word_freq_free  word_freq_business  word_freq_email  word_freq_you  \\\n",
       "count     4601.000000         4601.000000      4601.000000    4601.000000   \n",
       "mean         0.248848            0.142586         0.184745       1.662100   \n",
       "std          0.825792            0.444055         0.531122       1.775481   \n",
       "min          0.000000            0.000000         0.000000       0.000000   \n",
       "25%          0.000000            0.000000         0.000000       0.000000   \n",
       "50%          0.000000            0.000000         0.000000       1.310000   \n",
       "75%          0.100000            0.000000         0.000000       2.640000   \n",
       "max         20.000000            7.140000         9.090000      18.750000   \n",
       "\n",
       "       word_freq_credit  word_freq_your  word_freq_font  word_freq_000  \\\n",
       "count       4601.000000     4601.000000     4601.000000    4601.000000   \n",
       "mean           0.085577        0.809761        0.121202       0.101645   \n",
       "std            0.509767        1.200810        1.025756       0.350286   \n",
       "min            0.000000        0.000000        0.000000       0.000000   \n",
       "25%            0.000000        0.000000        0.000000       0.000000   \n",
       "50%            0.000000        0.220000        0.000000       0.000000   \n",
       "75%            0.000000        1.270000        0.000000       0.000000   \n",
       "max           18.180000       11.110000       17.100000       5.450000   \n",
       "\n",
       "       word_freq_money  word_freq_hp  word_freq_hpl  word_freq_george  \\\n",
       "count      4601.000000   4601.000000    4601.000000       4601.000000   \n",
       "mean          0.094269      0.549504       0.265384          0.767305   \n",
       "std           0.442636      1.671349       0.886955          3.367292   \n",
       "min           0.000000      0.000000       0.000000          0.000000   \n",
       "25%           0.000000      0.000000       0.000000          0.000000   \n",
       "50%           0.000000      0.000000       0.000000          0.000000   \n",
       "75%           0.000000      0.000000       0.000000          0.000000   \n",
       "max          12.500000     20.830000      16.660000         33.330000   \n",
       "\n",
       "       word_freq_650  word_freq_lab  word_freq_labs  word_freq_telnet  \\\n",
       "count    4601.000000    4601.000000     4601.000000       4601.000000   \n",
       "mean        0.124845       0.098915        0.102852          0.064753   \n",
       "std         0.538576       0.593327        0.456682          0.403393   \n",
       "min         0.000000       0.000000        0.000000          0.000000   \n",
       "25%         0.000000       0.000000        0.000000          0.000000   \n",
       "50%         0.000000       0.000000        0.000000          0.000000   \n",
       "75%         0.000000       0.000000        0.000000          0.000000   \n",
       "max         9.090000      14.280000        5.880000         12.500000   \n",
       "\n",
       "       word_freq_857  word_freq_data  word_freq_415  word_freq_85  \\\n",
       "count    4601.000000     4601.000000    4601.000000   4601.000000   \n",
       "mean        0.047048        0.097229       0.047835      0.105412   \n",
       "std         0.328559        0.555907       0.329445      0.532260   \n",
       "min         0.000000        0.000000       0.000000      0.000000   \n",
       "25%         0.000000        0.000000       0.000000      0.000000   \n",
       "50%         0.000000        0.000000       0.000000      0.000000   \n",
       "75%         0.000000        0.000000       0.000000      0.000000   \n",
       "max         4.760000       18.180000       4.760000     20.000000   \n",
       "\n",
       "       word_freq_technology  word_freq_1999  word_freq_parts  word_freq_pm  \\\n",
       "count           4601.000000     4601.000000      4601.000000   4601.000000   \n",
       "mean               0.097477        0.136953         0.013201      0.078629   \n",
       "std                0.402623        0.423451         0.220651      0.434672   \n",
       "min                0.000000        0.000000         0.000000      0.000000   \n",
       "25%                0.000000        0.000000         0.000000      0.000000   \n",
       "50%                0.000000        0.000000         0.000000      0.000000   \n",
       "75%                0.000000        0.000000         0.000000      0.000000   \n",
       "max                7.690000        6.890000         8.330000     11.110000   \n",
       "\n",
       "       word_freq_direct  word_freq_cs  word_freq_meeting  word_freq_original  \\\n",
       "count       4601.000000   4601.000000        4601.000000         4601.000000   \n",
       "mean           0.064834      0.043667           0.132339            0.046099   \n",
       "std            0.349916      0.361205           0.766819            0.223812   \n",
       "min            0.000000      0.000000           0.000000            0.000000   \n",
       "25%            0.000000      0.000000           0.000000            0.000000   \n",
       "50%            0.000000      0.000000           0.000000            0.000000   \n",
       "75%            0.000000      0.000000           0.000000            0.000000   \n",
       "max            4.760000      7.140000          14.280000            3.570000   \n",
       "\n",
       "       word_freq_project  word_freq_re  word_freq_edu  word_freq_table  \\\n",
       "count        4601.000000   4601.000000    4601.000000      4601.000000   \n",
       "mean            0.079196      0.301224       0.179824         0.005444   \n",
       "std             0.621976      1.011687       0.911119         0.076274   \n",
       "min             0.000000      0.000000       0.000000         0.000000   \n",
       "25%             0.000000      0.000000       0.000000         0.000000   \n",
       "50%             0.000000      0.000000       0.000000         0.000000   \n",
       "75%             0.000000      0.110000       0.000000         0.000000   \n",
       "max            20.000000     21.420000      22.050000         2.170000   \n",
       "\n",
       "       word_freq_conference  char_freq_;  char_freq_(  char_freq_[  \\\n",
       "count           4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean               0.031869     0.038575     0.139030     0.016976   \n",
       "std                0.285735     0.243471     0.270355     0.109394   \n",
       "min                0.000000     0.000000     0.000000     0.000000   \n",
       "25%                0.000000     0.000000     0.000000     0.000000   \n",
       "50%                0.000000     0.000000     0.065000     0.000000   \n",
       "75%                0.000000     0.000000     0.188000     0.000000   \n",
       "max               10.000000     4.385000     9.752000     4.081000   \n",
       "\n",
       "       char_freq_!  char_freq_$  char_freq_#  capital_run_length_average  \\\n",
       "count  4601.000000  4601.000000  4601.000000                 4601.000000   \n",
       "mean      0.269071     0.075811     0.044238                    5.191515   \n",
       "std       0.815672     0.245882     0.429342                   31.729449   \n",
       "min       0.000000     0.000000     0.000000                    1.000000   \n",
       "25%       0.000000     0.000000     0.000000                    1.588000   \n",
       "50%       0.000000     0.000000     0.000000                    2.276000   \n",
       "75%       0.315000     0.052000     0.000000                    3.706000   \n",
       "max      32.478000     6.003000    19.829000                 1102.500000   \n",
       "\n",
       "       capital_run_length_longest  capital_run_length_total      is_spam  \n",
       "count                 4601.000000               4601.000000  4601.000000  \n",
       "mean                    52.172789                283.289285     0.394045  \n",
       "std                    194.891310                606.347851     0.488698  \n",
       "min                      1.000000                  1.000000     0.000000  \n",
       "25%                      6.000000                 35.000000     0.000000  \n",
       "50%                     15.000000                 95.000000     0.000000  \n",
       "75%                     43.000000                266.000000     1.000000  \n",
       "max                   9989.000000              15841.000000     1.000000  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the spambase.csv as a pandas DataFrame (last column of data contains Target Data - is_spam)\n",
    "spam_data = pd.read_csv(\"../../data/spambase.csv\")\n",
    "  \n",
    "## For quick exploration:\n",
    "# spam_data.head()\n",
    "spam_data.describe()\n",
    "# spam_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 1: Explore the dataset and display some visualizations showing how the variables relate to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: There are 58 features here so you might want to reduce the dimensionality in subsequent steps*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 2: Build a simple logistic regression and visualize it\n",
    "\n",
    "use the variable \"capital_run_length_longest\" to predict \"is_spam\"\n",
    "How accurate is this single feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use train-test spit to split your data at a 30% mark and run another logistic regression using all variables\n",
    "\n",
    "Be sure to use random state = 12 so that we can compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 4: Apply cross-validation to see how the model fares across different splits of your date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use crossvalidation to score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Compare Performance of Logistic Regression to KNN with 3 neighbors\n",
    "-Which model is more accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Evaluate Feature Importance\n",
    "Which features are the most influential in this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 57)\n"
     ]
    }
   ],
   "source": [
    "target_name = spam_data.columns[-1]\n",
    "feature_names = spam_data.columns - [target_name]\n",
    "\n",
    "X = spam_data[feature_names]\n",
    "y = spam_data[target_name]\n",
    "\n",
    "# f_scores, p_values = f_classif(X, y)\n",
    "# print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931933381608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# print X_train.shape\n",
    "# print X_test.shape\n",
    "# print y_train.shape\n",
    "# print y_test.shape\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_clf = LogisticRegression()\n",
    "\n",
    "fit = logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "print fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# y_pred = logreg_clf.transform(X_test)\n",
    "y_pred = fit.predict(X_test) # [0, 1]\n",
    "\n",
    "y_pred_probs = fit.predict_proba(X_test) # [0 - 1]\n",
    "\n",
    "probs = pd.DataFrame(np.c_[y_pred_probs, y_pred, y_test], columns=['p(y=0)', 'p(y=1)', 'y_pred', 'y_test'])\n",
    "\n",
    "# probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926008938697\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAECCAYAAAAFL5eMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+n+wTuhj/+BntgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1174fc650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "false_positive_rate, true_positive_rate, _ = metrics.roc_curve(y_test, probs['p(y=1)'])\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "\n",
    "# from sklearn.cross_validation\n",
    "\n",
    "print metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90978886756237998"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(probs['y_pred'], probs['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a plot which shows classification accuracy as a function of number of features (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Plot the ROC Curve for the logistic regression you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Demonstrate how the accuracy of your predictions changes when you set your CV threshold to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 9: Discuss the pro's/con's of moving the threshold away from 50%, why is/isn't this a good idea? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explore and use your own reasoning to improve upon your results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the highest score you achieve on the training data, as measured by the area under your AUC curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bonus: \n",
    "\n",
    "Work through the example in the `SupportVectorMachines` folder and repeat the above steps using support vector machines. You'll also find the documentation on the SVC classifier in SkLearn: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html to be useful.\n",
    "\n",
    "For computational efficiency, keep the threshold at 50% and use the top K features calculated in Exercise 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary with the two features that best predict whether an email is spam or not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your visualization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own: \n",
    "\n",
    "Answer the following questions:\n",
    "    \n",
    "1. Explain which techniques did well and which techniques did not and your reasoning behind why they did/did not perform well.\n",
    "2. Between Logistic Regression and KNN, which classifier performed better? Why do you think this is?\n",
    "3. How might you improve this model?\n",
    "4. How does the number of features you used influence the performance and overfitting?\n",
    "\n",
    "**Submit your answers the above and to the exercises to the class repository, individually**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
