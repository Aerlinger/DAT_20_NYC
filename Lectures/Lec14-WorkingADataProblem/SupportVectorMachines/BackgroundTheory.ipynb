{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Support Vector Machines\n",
      "\n",
      "## Objectives\n",
      "\n",
      "- Linear Algebra Review\n",
      "- Get familiar with the theory behind Support Vector Machines\n",
      "- Learn how to Optimize with our 'C' variable\n",
      "- Work with Kernels  & more optimizations\n",
      "\n",
      "##Definitions:\n",
      "\n",
      "term / variable | definition\n",
      "--------|----------\n",
      "Vector | a quantity having direction and magnitude\n",
      "Hyperplane |  a subspace of one dimension less than the ambient space, which allows us to split our space.\n",
      "Kernel | function that enables us to transform our euclidean geometry in some way.  (Think similiarity function ~ dot products between vectors). \n",
      "$$b$$| intercept (hyperplane translation from origin)\n",
      "$$\\vec{w}$$ | normal vector (determines hyperplane orientation)\n",
      "$$\\vec{x_i}$$ | sample vector\n",
      "$$\\alpha_i$$ | Lagrangian weight\n",
      "$$C $$|  Regularization Term ( Allows us to vary our number of support vectors, ie margin of our 'street') (default = 1)\n",
      "$$epsilon$$ | our slack variable\n",
      "$$kernel$$ |  scikit learn parameter. Specify kernel of choice : 'linear','rbf','poly' (default is rbf)\n",
      "$$order$$ | degree of polynomial kernel function (default is 3)\n",
      "$$gamma$$ | Kernel gaussian coefficient for 'rbf' kernel (default is 0) \n",
      "\n",
      "reference : http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
      "\n",
      "# Class Notes\n",
      "\n",
      "\n",
      "\n",
      "## How would we split this dataset?  \n",
      "\n",
      "Figure 1 \n",
      "<img src='img/iris_split1.png'/>\n",
      "\n",
      "\n",
      "## And How would we split THIS dataset? \n",
      "\n",
      "Figure 2 \n",
      "\n",
      "<img src='img/iris_split2.png'/>\n",
      "\n",
      "It is not obvious how to divide the above data set into two distinct classes.\n",
      "  \n",
      "We will learn how. \n",
      "\n",
      "But first let's start with Linear Algebra review. \n",
      "\n",
      "\n",
      "### Basic operations on vectors in R^n\n",
      "\n",
      "\n",
      "In order to visualize the separation of our data (above) : we have restricted our data to two features.  Let's look at our two classes data points from distinct classes: \n",
      "\n",
      "Figure 3 \n",
      "\n",
      "<img src='img/Iris_split3.png'/>\n",
      "\n",
      "\n",
      "These data points are vectors.  (Note: if we were using all 4 features of the data, each vector would be represented as a 4 x 1 matrix (in lieu of 2 x 1))\n",
      "\n",
      "\n",
      "u = $\\begin{bmatrix}5\\\\3\\end{bmatrix}$\n",
      "\n",
      "v = $\\begin{bmatrix}5.4\\\\3\\end{bmatrix}$\n",
      "\n",
      "\n",
      "### Multiplication by a scalar:\n",
      "When we multiply a vector by a scalar, we are 'stretching' it (if scalar is pos. and 'shrinking' the vector if the scalar is neg.)\n",
      "\n",
      "c= 2\n",
      "cu =  ? \n",
      "\n",
      "\n",
      "### Vectors Inner Products \n",
      "\n",
      "Now let's find the dot product of u & v:\n",
      "\n",
      "u.v = $\\begin{bmatrix}5\\\\3\\end{bmatrix}^\\top$ $\\begin{bmatrix}5.4\\\\3\\end{bmatrix}$= $\\begin{bmatrix}5, 3\\end{bmatrix}$ $\\begin{bmatrix}5.4\\\\3\\end{bmatrix}$\n",
      "\n",
      "$u_1v_1 + u_2v_2$ =  ?\n",
      "\n",
      "### Dot Product Visualization\n",
      "\n",
      "We can visualize the dot product by taking v and orthogonally projecting onto u\n",
      "\n",
      "\n",
      "<img src='img/projection.png'/>\n",
      "\n",
      "\n",
      "### Norms: \n",
      "<img src='img/norm.png'/>\n",
      "\n",
      "Another property that we should be familiar with is the norm.\n",
      "Norm = Euclidean Length (or Hypotheneuse) as shown above.\n",
      "\n",
      "Norm : $${\\|u\\|}= \\sqrt{((5)^2 + (3)^2)}$$\n",
      "\n",
      "### Vector Addition / Subtraction\n",
      "\n",
      "u + v = ? \n",
      "\n",
      "### Theory Behind SVM\n",
      "\n",
      "Again lets consider our Iris Example.  In order to make the math work for us, we will consider one class to have a target value of (+1) and the other to have a value of (-1). \n",
      "\n",
      "- We know we need to draw a line (or hyperplane) to split these classes, but which line do we choose?   A line could have any orientation (w) and any displacement from origin (b).  We will find out how to develop the equation for the optimal Hyperplane Line.  \n",
      "\n",
      "- We want to add a line (while keeping in mind) that we would like to add the widest 'street' (margin) possible.  We will also learn how to optimize our model by adjusting the width of our margin.\n",
      "\n",
      "\n",
      "####EQN #1) Our Decision Rule:\n",
      "\n",
      "Imagine we have a normal vector w (perpendicular to the margin) and some unknown vector x:\n",
      "In order to get a sense of which category (side of the street) our unknown vector is: we find the dot product of w and x ( if large enough, the unknown will be on the 'pos' side of street).\n",
      "\n",
      "$$\\vec{w}\\cdot\\vec{x} == ? $$ \n",
      "\n",
      "$$\\vec{w}\\cdot\\vec{x} +b \\geq 0$$   \n",
      "Then: positive sample\n",
      "\n",
      "\n",
      "#### EQN #2) For our x's in the margins of our 'street':\n",
      "\n",
      "$${y}_{i}(\\vec{w}\\cdot\\vec{x_+}+b)>=1$$ \n",
      "$${y}_{i}(\\vec{w}\\cdot\\vec{x_-}+b)<= -1$$ \n",
      "\n",
      "** (and for all x in in the margin of our street:)\n",
      "$${y}_{i}(\\vec{w}\\cdot\\vec{x}+b)-1= 0$$ \n",
      "\n",
      "**  Here we want to note: that 1 is arbritrary (mathematically convenient) \n",
      "  \n",
      "- Remember we are trying to develop a street as wide as possible (We will see later this, is an optimiz..)\n",
      "\n",
      "- We can find the differences of two vectors: (but this isn't our width).\n",
      "\n",
      "\n",
      "#### EQN #3)  Finding the width of our street:\n",
      " $$width =({x}_{+}-{x}_{-})\\cdot{{\\vec{w}\\above 1pt\\|w\\|}}$$\n",
      "\n",
      " $$ unit vector =  {{\\vec{w}\\above 1pt\\|w\\|}}$$\n",
      "\n",
      "#### EQN #4)  Manipulating equations 2 & 3 we have:\n",
      "\n",
      "\n",
      "$${x}_{+}{{\\vec{w}\\above 1pt\\|w\\|}}-{x}_{-}{{\\vec{w}\\above 1pt\\|w\\|}}==$$\n",
      "\n",
      "$${{(1-b\\above 1pt\\|w\\|}}  - {{(-1-b)\\above 1pt\\|w\\|}}$$\n",
      "\n",
      "We need to maximize the following in order to get our widest street: \n",
      "\n",
      " $$MAX  {{2\\above 1pt\\|w\\|}}$$\n",
      "\n",
      "We see that the width is inversely proportional to the length of the normal vector\n",
      "  \n",
      "##### EQN #5)  Lagrange Multipliers:\n",
      "If we want to find the maximum of a function (that has restraints), then we need to use a technique called: Lagrange Multipliers. \n",
      "Regarranging equation 4 and inserting our Lagrange Mulipliers (alpha) we have:\n",
      "\n",
      "  $$ L=  1/2{{\\|w\\|}^2} - \\sum \\alpha_i[(y_i(\\vec{w}\\cdot{x_i} +b)-1] $$\n",
      "  \n",
      "  \n",
      "####EQNs #6)   And how do we find maxima ?   Take the derivative \n",
      "- taking partial derivative of above (with respect to w), we find that :\n",
      "\n",
      "  $$\\vec{w} = \\sum \\alpha_i(y_i{x_i})$$\n",
      "  \n",
      "- And now we see that vector w is just a linear sum of our samples!\n",
      "\n",
      "- And taking partial derivative of above (with respect to b) we have:\n",
      "\n",
      "  $$ \\sum \\alpha_i(y_i)=0$$\n",
      "  \n",
      "- We discover that our optimization only depends on (what? )\n",
      " \n",
      "  $$ L= \\sum \\alpha_i -1/2\\sum\\sum\\alpha_i\\alpha_j(y_iy_j)(x_i\\cdot{x_j})$$\n",
      "  \n",
      "  \n",
      "- The Dot product of pairs of samples!\n",
      " \n",
      "####EQNs #7)   Our final Hyperplane Equation\n",
      "\n",
      "-  And now we see that our Decision Rule depends only on our sample vectors and u (the unknown vector) \n",
      " \n",
      "  $$ \\sum\\alpha_iy_i(x_i\\cdot\\vec{u}) + b \\geq\\ 0$$\n",
      "  \n",
      " Again, our solutions have total dependence on the Dot Products.\n",
      " \n",
      " It still doesn't seem possible that we can find a solution for the dataset above, using just the above equations .. right ?   \n",
      "\n",
      "Let's try a trivial example:\n",
      "\n",
      "### Trivial Example:\n",
      "\n",
      "- Lets take our 2 points from Figure 3.  These points are our support vectors\n",
      "- We would like to discover a simple SVM that accurately discriminates between \n",
      "our two classes.\n",
      "\n",
      " $ x_1= \\begin{bmatrix}5.4\\\\3\\end{bmatrix}$  (positive sample)\n",
      "\n",
      " $ x_2 =\\begin{bmatrix}5\\\\3\\end{bmatrix}$    (negative sample)\n",
      "\n",
      "In order to account for our bias input, we can adjust our vectors as follows:\n",
      "    \n",
      " $ {x_1}= \\begin{bmatrix}5.4\\\\3\\\\1\\end{bmatrix}$\n",
      " \n",
      " $ x_2 =\\begin{bmatrix}5\\\\3\\\\1\\end{bmatrix}$\n",
      " \n",
      " Using the structure we developed in our final EQN 7, we have:\n",
      " \n",
      "  $$\\alpha_1(x_1\\cdot(x_1)) + \\alpha_2(x_1\\cdot(x_2)) = +1 $$\n",
      "  $$\\alpha_1(x_1\\cdot(x_2)) + \\alpha_2(x_2\\cdot(x_2)) = -1 $$\n",
      "  \n",
      "  $$ (39.16)\\alpha_1 + (37)\\alpha_2 = +1 $$\n",
      "  $$ (37)\\alpha_1 + (35)\\alpha_2 = - 1 $$\n",
      "  \n",
      "  A little algebra will give us a solution to the system of equations:\n",
      "  \n",
      "  $$\\alpha_1= 2.18$$\n",
      "  $$\\alpha_2 =-1.25$$\n",
      "  \n",
      "  \n",
      "  Using EQN 6), we can find our hyperplane \n",
      "  \n",
      "  \n",
      "   $$\\vec{w} = \\sum \\alpha_i({x_i})$$\n",
      "   \n",
      "   =$$2.18\\begin{bmatrix}5.4\\\\3\\\\1\\end{bmatrix}  - 1.25\\begin{bmatrix}5\\\\3\\\\1\\end{bmatrix} =$$\n",
      "   \n",
      "   $$\\begin{bmatrix}5.522\\\\2.79\\\\.93 \\end{bmatrix}$$\n",
      "   \n",
      "   Alas, the above vector describes our separating hyperplane equation ( (with the last value as our bias)\n",
      "   \n",
      "   y = wx + b , where w = $$\\begin{bmatrix}5.522\\\\2.79\\end{bmatrix}$$\n",
      "   and b = 0.93\n",
      "   \n",
      "   \n",
      "   Finally: we can visualize our Hyperplane!\n",
      "   \n",
      "\n",
      "   \n",
      "   <img src='img/iris_split4.png'/>\n",
      "\n",
      "### Thinking out loud: \n",
      "\n",
      "-  Which parameters determine our hyperplane ?\n",
      "\n",
      "-  What Support Vectors did we use ?\n",
      "\n",
      "-  What are the advantages of using few support vectors vs. many?\n",
      "\n",
      "-  Which hyperparameter will allow us to adjust our number of support vectors?\n",
      "\n",
      "-  How do you feel SVMs might behave in regards to outliers ? \n",
      "\n"
     ]
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": "",
  "signature": "sha256:252d02a995c89d5a4fdaeb93638fa19f48b7df88ec7ec32a0e2f65a36d4815c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0
}