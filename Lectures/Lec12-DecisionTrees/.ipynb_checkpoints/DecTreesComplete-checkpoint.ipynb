{
 "metadata": {
  "name": "",
  "signature": "sha256:36e19ae8f01b036ec24ba0fe7cbcfc8b6545b8e48b302eb255305374a86fb359"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to Decision Trees\n",
      "\n",
      "> If there are two courses of action, you should take the third.\n",
      "\n",
      "<footer>Jewish Proverb</footer>\n",
      "\n",
      "## What is a decision tree?\n",
      "\n",
      "A decision tree is a flowchart-like structure in which each internal node represents a test of an attribute, each branch represents an outcome of that test and each leaf node represents class label (a decision taken after testing all attributes in the path from the root to the leaf). Each path from the root to a leaf can also be represented as a classification rule.\n",
      "\n",
      "## Properties of a decision tree:\n",
      "\n",
      "* **Non-parametric**: no parameters, no distribution assumptions\n",
      "* **Hierarchical**: consists of a sequence of questions which yield a class label when applied to any record\n",
      "* **Variable Size**: Any boolean functions can be represented\n",
      "* **Deterministic**: For the same set of features the tree will assign the same label\n",
      "* Support for **Discrete** and **Continuous** Parameters:\n",
      "  * Binning and Threshold\n",
      "\n",
      "### Let's consider a simple example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"./img/vertedrate_dataset.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**In a decision tree, the nodes represent questions (test conditions) and the edges are the answers to these questions.**\n",
      "\n",
      "* *Edge*, lead from one a _parent_ to _child_ nodes\n",
      "* *Root*, node has 0 incoming edges, and 2+ outgoing edges.\n",
      "* *Leaf*, has 1 incoming edge and, 0 outgoing edges. Leaf nodes\n",
      "correspond to class labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"./img/decision_tree_mammal.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Let's analyze another very simple dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "weather = pd.read_csv('../../Data/weather.csv')\n",
      "\n",
      "weather"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>outlook</th>\n",
        "      <th>temperature</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windy</th>\n",
        "      <th>play</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>    sunny</td>\n",
        "      <td>  hot</td>\n",
        "      <td>   high</td>\n",
        "      <td> False</td>\n",
        "      <td>  no</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>    sunny</td>\n",
        "      <td>  hot</td>\n",
        "      <td>   high</td>\n",
        "      <td>  True</td>\n",
        "      <td>  no</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> overcast</td>\n",
        "      <td>  hot</td>\n",
        "      <td>   high</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>    rainy</td>\n",
        "      <td> mild</td>\n",
        "      <td>   high</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>    rainy</td>\n",
        "      <td> cool</td>\n",
        "      <td> normal</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>    rainy</td>\n",
        "      <td> cool</td>\n",
        "      <td> normal</td>\n",
        "      <td>  True</td>\n",
        "      <td>  no</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> overcast</td>\n",
        "      <td> cool</td>\n",
        "      <td> normal</td>\n",
        "      <td>  True</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>    sunny</td>\n",
        "      <td> mild</td>\n",
        "      <td>   high</td>\n",
        "      <td> False</td>\n",
        "      <td>  no</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>    sunny</td>\n",
        "      <td> cool</td>\n",
        "      <td> normal</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>    rainy</td>\n",
        "      <td> mild</td>\n",
        "      <td> normal</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>    sunny</td>\n",
        "      <td> mild</td>\n",
        "      <td> normal</td>\n",
        "      <td>  True</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> overcast</td>\n",
        "      <td> mild</td>\n",
        "      <td>   high</td>\n",
        "      <td>  True</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> overcast</td>\n",
        "      <td>  hot</td>\n",
        "      <td> normal</td>\n",
        "      <td> False</td>\n",
        "      <td> yes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>    rainy</td>\n",
        "      <td> mild</td>\n",
        "      <td>   high</td>\n",
        "      <td>  True</td>\n",
        "      <td>  no</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "     outlook temperature humidity  windy play\n",
        "0      sunny         hot     high  False   no\n",
        "1      sunny         hot     high   True   no\n",
        "2   overcast         hot     high  False  yes\n",
        "3      rainy        mild     high  False  yes\n",
        "4      rainy        cool   normal  False  yes\n",
        "5      rainy        cool   normal   True   no\n",
        "6   overcast        cool   normal   True  yes\n",
        "7      sunny        mild     high  False   no\n",
        "8      sunny        cool   normal  False  yes\n",
        "9      rainy        mild   normal  False  yes\n",
        "10     sunny        mild   normal   True  yes\n",
        "11  overcast        mild     high   True  yes\n",
        "12  overcast         hot   normal  False  yes\n",
        "13     rainy        mild     high   True   no"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = weather.drop('play', axis=1)\n",
      "\n",
      "# Let's convert the target variable to numerical so it's easier to work with:\n",
      "y = weather['play'].map({'yes': 1, 'no': 0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a very basic where we want to determine whether or not to play outside based on 4 categorical variables related to weather:\n",
      "\n",
      "\n",
      "#### Feature variables (All categorical):\n",
      "```\n",
      "- Outlook: [sunny|overcast|rainy]\n",
      "- Temperature: [hot|mild|cool]\n",
      "- humidity: [high|normal]\n",
      "- windy: [True|False]\n",
      "```\n",
      "    \n",
      "#### Target: \n",
      "```\n",
      "- play: [True|False]\n",
      "```\n",
      "    \n",
      "**Q:** How might we build the **simplest** learning procedure to predict whether or not we play?\n",
      "\n",
      "**A:** *start by evaluating **one feature at a time** *\n",
      "\n",
      "Comparing `humidity` to `play`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "humidity_df = pd.DataFrame(np.c_[X['humidity'], y], columns=['humidity', 'play'])\n",
      "\n",
      "## There are only factors for humidy ['high', 'normal']\n",
      "# As a simple rule, let's try play if humidity == 'normal':\n",
      "\n",
      "humidity_df['pred_play'] = (humidity_df['humidity'] == 'normal').astype(np.int)\n",
      "humidity_df['error'] = (humidity_df['play'] != humidity_df['pred_play']).astype(np.int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Great, so we have a very basic classifier! \n",
      "\n",
      "<img src=\"img/humidity_tree.png\">\n",
      "\n",
      "*Now let's evaluate our classifier using the metrics from previous classes.* \n",
      "\n",
      "### Your Turn:\n",
      "\n",
      "#### 1. In groups of two compute the following **without sklearn**:\n",
      "    - Compute the number TP (True Pos.), TN (True Neg.), FP, FN\n",
      "    - Generate the confusion matrix\n",
      "    - Compute the Accuracy\n",
      "    - Compute the Precision\n",
      "    - Compute the Recall\n",
      "    - Compute the F1 score"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"img/confusion_matrix_metrics.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your Code Here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2. Verify the above results using SKlearn\n",
      "\n",
      "*Hint: There's a module in sklearn that we used in the previous two classes lets us do this very easily*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Your Turn! Repeat the above process for each additional feature "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"img/splits.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How to build a decision tree\n",
      "\n",
      "1. Pick the *best* attribute\n",
      "2. Ask a question\n",
      "3. Follow the answer path\n",
      "4. Go to 1 until we have an answer\n",
      "\n",
      "**Question:** Above we got *multiple decision trees*, but which one do you think is best?\n",
      "\n",
      "The truth is, we can classify a dataset using **many** number of decision trees, how do we derive the best one?\n",
      "\n",
      "One possibility would be to evaluate all possible decision trees (eg, all permutations of test conditions) for a given dataset. But this is generally too complex to be practical \u00e0 $ \\rightarrow O(2^n)$.\n",
      "\n",
      "We can find a practical solution that works, by using a **heuristic** algorithm. In other words, given a set of options we make the best decision we can and hope for the best, rather than exhaustively search every possible solution. Our solution isn't guaranteed to be perfect, but the time and complexity requirement is far less than inductively finding the ideal solution.\n",
      "\n",
      "One basic method used to build (or \u201cgrow\u201d) a decision tree is **Hunt\u2019s\n",
      "algorithm**.\n",
      "\n",
      "Hunt\u2019s is a greedy recursive algorithm that leads to a local optimum. It builds a decision tree by recursively partitioning records into smaller & smaller subsets.\n",
      "\n",
      "\n",
      "* greedy \u2013 algorithm makes locally optimal decision at each step\n",
      "* recursive \u2013 splits task into subtasks, solves each the same way\n",
      "* local optimum \u2013 solution for a given neighborhood of points\n",
      "\n",
      "\n",
      "The partitioning decision is made at each node according to a metric called *purity*. **A partition is 100% pure when all of its records belong to a single class** (e.g. if an animal gives birth it's a mammal, otherwise, it's not). Let $D_t$ be the set of training records that reach a node $t$. The general recursive procedure is defined as below:\n",
      "\n",
      "* If $D_t$ only contains records that belong the same class $y_t$, then $t$ is a leaf node labeled as $y_t$\n",
      "* If $D_t$ is an empty set, then $t$ is a leaf node labeled by the default class, $y_d$\n",
      "* If $D_t$ contains records that belong to more than one class, use an attribute test to split the data into smaller subsets.\n",
      "\n",
      "It recursively applies the procedure to each subset until all the records in the subset belong to the same class. The Hunt's algirithm assumes that each combination of attribute sets has a unique class label during the procedure. If all the records associated with $D_t$ have identical attribute values except for the class label, then it is not possible to split these records any further. In this case, the node is declared a leaf node with the same class label as the majority class of training records associated with this node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partitions\n",
      "\n",
      "* *Binary attributes* : leads to two-way split test condition.\n",
      "\n",
      "![](img/dt_binary.png)\n",
      "\n",
      "* *Nominal attributes* : the test condition can be expressed into multiway split on each distinct values, or two-way split by grouping the attribute values into two subsets.\n",
      "\n",
      "![](img/dt_nominal.png)\n",
      "\n",
      "* *Ordinal attributes* : can also produce binary or multiway splits as long as the grouping does not violate the order property of the attribute values.\n",
      "\n",
      "![](img/dt_ordinal.png)\n",
      "\n",
      "* *Continuous attributes* : The test condition can be expressed as a comparsion test with two outcomes, or a range query. Or we can discretize the continous value into nominal attribute and then perform two-way or multi-way split.\n",
      "\n",
      "![](img/dt_continuous.png)\n",
      "\n",
      "###Purity\n",
      "\n",
      "**The hardest part about building the tree is selecting the best attribute test condition**, in other words, the best split. There are three common impurity measures used to measure the best split. Since the goal of a decision tree is to have nodes consisting entirely of members of a single class, the impurity of a node is the extent to which that is not the case. \n",
      "\n",
      "For example, a node with 2 members of one class, and 0 members of another class has zero impurity. A node with 1 member of one class, and one of another, however, has the highest impurity. \n",
      "\n",
      "The three most common measures of impurity are **entropy**, **gini impurity**, and **classification error**. They are defined using the following equations, where $p(i|t)$ denotes the fraction of records that belong to class $i$ at a given node $t$, and $c$ is the number of classes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "node_a = (12,8)\n",
      "node_b = (2,18)\n",
      "purity = lambda x: np.max(x) / np.sum(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Entropy(t)=-\\sum\\limits_{i=0}^{c-1}p(i|t)\\log_{2} p(i|t)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"purity @ node_a :\", purity(node_a)\n",
      "print \"purity @ node_b :\", purity(node_b)\n",
      "entropy_a = -1 * (purity(node_a) * np.log2(purity(node_a)))\n",
      "entropy_b = -1 * (purity(node_b) * np.log2(purity(node_b)))\n",
      "\n",
      "print \"entroy of split :\", entropy_a\n",
      "print \"entroy of split :\", entropy_b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "purity @ node_a : 0.6\n",
        "purity @ node_b : 0.9\n",
        "entroy of split : 0.4421793565\n",
        "entroy of split : 0.136802784101\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Gini(t)=1-\\sum\\limits_{i=0}^{c} [p(i|t)]^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"purity @ node_a :\", purity(node_a)\n",
      "print \"purity @ node_b :\", purity(node_b)\n",
      "gini_a = 1 - np.sum([np.square(node) for node in [purity(node_a), 1 - purity(node_a)]])\n",
      "gini_b = 1 - np.sum([np.square(node) for node in [purity(node_b), 1 - purity(node_b)]])\n",
      "\n",
      "print \"gini of node_a :\", gini_a\n",
      "print \"gini of node_b :\", gini_b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "purity @ node_a : 0.6\n",
        "purity @ node_b : 0.9\n",
        "gini of node_a : 0.48\n",
        "gini of node_b : 0.18\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ Classification\\ error(t)=1-\\max_{i}[p(i|t)] $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"purity @ node_a :\", purity(node_a)\n",
      "print \"purity @ node_b :\", purity(node_b)\n",
      "class_error_a = 1 - purity(node_a)\n",
      "class_error_b = 1 - purity(node_b)\n",
      "\n",
      "print \"classification error of node_a :\", class_error_a\n",
      "print \"classification error of node_b :\", class_error_b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "purity @ node_a : 0.6\n",
        "purity @ node_b : 0.9\n",
        "classification error of node_a : 0.4\n",
        "classification error of node_b : 0.1\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](img/impurity_measures.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The equation for information gain is:\n",
      "\n",
      "$$ \\Delta = I(parent) - \\sum\\limits_{j=1}^k \\frac{N(v_j)}{N}I(v_j) $$\n",
      "\n",
      "Where $I(\u22c5)$ is the impurity measure of a given node, $N$ is the total number or records at the given node's parent, $k$ is the number of attribute values, and $N(v_j)$ is the number of records associated with the child node, $v_j$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def purity(split):\n",
      "    return np.max(split)/np.sum(split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gini(split):\n",
      "    a = purity(split)\n",
      "    b = 1 - a\n",
      "    gini = 1 - np.sum(np.square(a) + np.square(b))\n",
      "    return gini"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parent_node(nodes):\n",
      "    return [np.sum(x) for x in zip(nodes)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def info_gain(nodes):\n",
      "    return gini(parent_node(nodes)) - sum([(np.sum(node)/np.sum(parent_node(nodes))) * gini(node) for node in nodes])\n",
      "nodes = [node_a,node_b]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def info_gain(nodes):\n",
      "    a = nodes[0]\n",
      "    b = nodes[1]\n",
      "    gini_parent = gini(parent_node(nodes))\n",
      "    x = (np.sum(a) / np.sum(parent_node(nodes))) * gini(a)\n",
      "    y = (np.sum(b) / np.sum(parent_node(nodes))) * gini(b)\n",
      "    return gini_parent - np.sum([x,y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ig = info_gain([node_a,node_b])\n",
      "print \"Information Gain: %.03f\" % ig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Information Gain: 0.170\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](img/dt_splitting_binary.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Splitting nominal attributes**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](img/dt_splitting_nominal.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gain Ratio\n",
      "\n",
      "Generally speaking, a test condition with a high number of outcomes\n",
      "can lead to overfitting (ex: a split with one outcome per record).\n",
      "One way of dealing with this is to restrict the algorithm to binary\n",
      "splits only (CART). Another way is to use a splitting criterion which explicitly penalizes the number of outcomes (C4.5). We can use a function of the information gain called the gain ratio to explicitly penalize high numbers of outcomes.\n",
      "\n",
      "Gain ratio is a modification of the information gain that reduces its bias on high-branch attributes. It will be\n",
      "\n",
      "* Large when data is evenly spread\n",
      "* Small when all data belong to one branch\n",
      "\n",
      "But the Gain Ratio also takes the number and size of branches into account when choosing an attribute. It corrects the information gain by taking the _intrinsic information_ of a split into account. That is, how much info do we need to tell which branch an instance belongs to.\n",
      "\n",
      "$$ Gain ratio = \\frac{\\Delta info}{Split Info} $$\n",
      "\n",
      "$$ Split Info = -\\sum\\limits_{i=0}^{k}P(v_i)log_2 P(v_i) $$\n",
      "\n",
      "Where $p(v_i)$ refers to the probability of label $i$ at node $v$ and $k$ is the total number of splits. For example, if each attribute value has the same number of records, then $\u2200_i : P(v_i) = 1/k $ and the split information would be equal to $log_{2}k$. This example suggests that if an attribute produces a large number of splits, its split information will also be large , which in turn reduces its gain ratio."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preventing Overfitting\n",
      "\n",
      "In addition to determining splits, we also need a stopping criterion to\n",
      "tell us when we\u2019re done. For example, we can stop when all records belong to the same class,\n",
      "or when all records have the same attributes. This is correct in principle, but would likely lead to overfitting.\n",
      "\n",
      "### pre-pruning\n",
      "\n",
      "One possibility is pre-pruning, which involves setting a minimum\n",
      "threshold on the gain, and stopping when no split achieves a gain\n",
      "above this threshold.\n",
      "\n",
      "This prevents overfitting, but is difficult to calibrate in practice (may\n",
      "preserve bias!)\n",
      "\n",
      "### post-pruning\n",
      "\n",
      "Alternatively we could build the full tree, and then perform pruning\n",
      "as a post-processing step.\n",
      "\n",
      "To prune a tree, we examine the nodes from the bottom-up and\n",
      "simplify pieces of the tree (according to some criteria).\n",
      "\n",
      "Complicated subtrees can be replaced either with a single node, or\n",
      "with a simpler (child) subtree.\n",
      "\n",
      "The first approach is called **subtree replacement**, and the second is\n",
      "**subtree raising**.\n",
      "\n",
      "![](img/dt_post_pruning.png)\n",
      "\n",
      "Generally, (or at least depending on your data), it can be very easy to overfit a model with decision trees.\n",
      "\n",
      "![](img/dt_overfitting.png)\n",
      "\n",
      "## Dealing with missing values\n",
      "\n",
      "* Imputing during training\n",
      "  * Most frequent one in the dataset\n",
      "  * Most frequent in its class\n",
      "  * Fractional Examples, proportional to the real distribution\n",
      "\n",
      "\n",
      "* Imputing during testing\n",
      "  * Voting by fractional leafs\n",
      "\n",
      "### Tree algorithms: ID3, C4.5 and CART\n",
      "\n",
      "What are all the various decision tree algorithms and how do they differ from each other? Scikit-learn uses an optimised version of the CART algorithm.\n",
      "\n",
      "* **ID3 (Iterative Dichotomiser 3)** algorithm builds tree based on the information (information gain) obtained from the training instances and then uses the same to classify the test data. ID3 algorithm generally uses nominal attributes for classification with no missing values. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
      "\n",
      "* **C4.5 is the successor to ID3** and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule\u2019s precondition if the accuracy of the rule improves without it.\n",
      "\n",
      "* **CART (Classification and Regression Trees)** is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing decision trees with SK Learn: \n",
      "\n",
      "Let's revisit the Titanic Data\n",
      "\n",
      "* Implement the decision tree classification to the test iris set\n",
      "* Review the implementation and output of a confusion matrix\n",
      "* Error terms: Precision and Recall"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the data\n",
      "titanic = pd.read_csv('../../data/titanic.csv')\n",
      "titanic.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>survived</th>\n",
        "      <th>pclass</th>\n",
        "      <th>name</th>\n",
        "      <th>sex</th>\n",
        "      <th>age</th>\n",
        "      <th>sibsp</th>\n",
        "      <th>parch</th>\n",
        "      <th>ticket</th>\n",
        "      <th>fare</th>\n",
        "      <th>cabin</th>\n",
        "      <th>embarked</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                           Braund, Mr. Owen Harris</td>\n",
        "      <td>   male</td>\n",
        "      <td> 22</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>        A/5 21171</td>\n",
        "      <td>  7.2500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
        "      <td> female</td>\n",
        "      <td> 38</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>         PC 17599</td>\n",
        "      <td> 71.2833</td>\n",
        "      <td>  C85</td>\n",
        "      <td> C</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td>                            Heikkinen, Miss. Laina</td>\n",
        "      <td> female</td>\n",
        "      <td> 26</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> STON/O2. 3101282</td>\n",
        "      <td>  7.9250</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td>      Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
        "      <td> female</td>\n",
        "      <td> 35</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           113803</td>\n",
        "      <td> 53.1000</td>\n",
        "      <td> C123</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                          Allen, Mr. William Henry</td>\n",
        "      <td>   male</td>\n",
        "      <td> 35</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           373450</td>\n",
        "      <td>  8.0500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                                  Moran, Mr. James</td>\n",
        "      <td>   male</td>\n",
        "      <td>NaN</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           330877</td>\n",
        "      <td>  8.4583</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> Q</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td>                           McCarthy, Mr. Timothy J</td>\n",
        "      <td>   male</td>\n",
        "      <td> 54</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>            17463</td>\n",
        "      <td> 51.8625</td>\n",
        "      <td>  E46</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                    Palsson, Master. Gosta Leonard</td>\n",
        "      <td>   male</td>\n",
        "      <td>  2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>           349909</td>\n",
        "      <td> 21.0750</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
        "      <td> female</td>\n",
        "      <td> 27</td>\n",
        "      <td> 0</td>\n",
        "      <td> 2</td>\n",
        "      <td>           347742</td>\n",
        "      <td> 11.1333</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td>               Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
        "      <td> female</td>\n",
        "      <td> 14</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           237736</td>\n",
        "      <td> 30.0708</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> C</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "   survived  pclass                                               name  \\\n",
        "0         0       3                            Braund, Mr. Owen Harris   \n",
        "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
        "2         1       3                             Heikkinen, Miss. Laina   \n",
        "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
        "4         0       3                           Allen, Mr. William Henry   \n",
        "5         0       3                                   Moran, Mr. James   \n",
        "6         0       1                            McCarthy, Mr. Timothy J   \n",
        "7         0       3                     Palsson, Master. Gosta Leonard   \n",
        "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n",
        "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)   \n",
        "\n",
        "      sex  age  sibsp  parch            ticket     fare cabin embarked  \n",
        "0    male   22      1      0         A/5 21171   7.2500   NaN        S  \n",
        "1  female   38      1      0          PC 17599  71.2833   C85        C  \n",
        "2  female   26      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
        "3  female   35      1      0            113803  53.1000  C123        S  \n",
        "4    male   35      0      0            373450   8.0500   NaN        S  \n",
        "5    male  NaN      0      0            330877   8.4583   NaN        Q  \n",
        "6    male   54      0      0             17463  51.8625   E46        S  \n",
        "7    male    2      3      1            349909  21.0750   NaN        S  \n",
        "8  female   27      0      2            347742  11.1333   NaN        S  \n",
        "9  female   14      1      0            237736  30.0708   NaN        C  "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Always check for missing values!\n",
      "titanic.isnull().sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "survived      0\n",
        "pclass        0\n",
        "name          0\n",
        "sex           0\n",
        "age         177\n",
        "sibsp         0\n",
        "parch         0\n",
        "ticket        0\n",
        "fare          0\n",
        "cabin       687\n",
        "embarked      2\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's choose our response and a few features, and decide whether we need to adjust them:\n",
      "\n",
      "- **survived:** This is our response, and is already encoded as 0=died and 1=survived.\n",
      "- **pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). Should we consider these ordered or nonordered?\n",
      "- **sex:** This is a binary category, so we should encode as 0=female and 1=male.\n",
      "- **age:** We need to fill in the missing values.\n",
      "- **embarked:** This is the port they embarked from. There are three unordered categories, so we'll create dummy variables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# copy\n",
      "titanic_c = pd.DataFrame(titanic)\n",
      "\n",
      "# encode sex feature\n",
      "titanic_c['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
      "\n",
      "# fill in missing values for age\n",
      "titanic_c.age.fillna(titanic.age.mean(), inplace=True)\n",
      "\n",
      "# is there a more intelligent way we might handle age?\n",
      "#titanic_c.age.fillna(, inplace=True)\n",
      "\n",
      "# print the updated DataFrame\n",
      "titanic_c.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>survived</th>\n",
        "      <th>pclass</th>\n",
        "      <th>name</th>\n",
        "      <th>sex</th>\n",
        "      <th>age</th>\n",
        "      <th>sibsp</th>\n",
        "      <th>parch</th>\n",
        "      <th>ticket</th>\n",
        "      <th>fare</th>\n",
        "      <th>cabin</th>\n",
        "      <th>embarked</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                           Braund, Mr. Owen Harris</td>\n",
        "      <td> 1</td>\n",
        "      <td> 22.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>        A/5 21171</td>\n",
        "      <td>  7.2500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 38.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>         PC 17599</td>\n",
        "      <td> 71.2833</td>\n",
        "      <td>  C85</td>\n",
        "      <td> C</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td>                            Heikkinen, Miss. Laina</td>\n",
        "      <td> 0</td>\n",
        "      <td> 26.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> STON/O2. 3101282</td>\n",
        "      <td>  7.9250</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td>      Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 35.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           113803</td>\n",
        "      <td> 53.1000</td>\n",
        "      <td> C123</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                          Allen, Mr. William Henry</td>\n",
        "      <td> 1</td>\n",
        "      <td> 35.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           373450</td>\n",
        "      <td>  8.0500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                                  Moran, Mr. James</td>\n",
        "      <td> 1</td>\n",
        "      <td> 29.699118</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           330877</td>\n",
        "      <td>  8.4583</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> Q</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td>                           McCarthy, Mr. Timothy J</td>\n",
        "      <td> 1</td>\n",
        "      <td> 54.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>            17463</td>\n",
        "      <td> 51.8625</td>\n",
        "      <td>  E46</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                    Palsson, Master. Gosta Leonard</td>\n",
        "      <td> 1</td>\n",
        "      <td>  2.000000</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>           349909</td>\n",
        "      <td> 21.0750</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 27.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 2</td>\n",
        "      <td>           347742</td>\n",
        "      <td> 11.1333</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td>               Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 14.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           237736</td>\n",
        "      <td> 30.0708</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> C</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "   survived  pclass                                               name  sex  \\\n",
        "0         0       3                            Braund, Mr. Owen Harris    1   \n",
        "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
        "2         1       3                             Heikkinen, Miss. Laina    0   \n",
        "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
        "4         0       3                           Allen, Mr. William Henry    1   \n",
        "5         0       3                                   Moran, Mr. James    1   \n",
        "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
        "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
        "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
        "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
        "\n",
        "         age  sibsp  parch            ticket     fare cabin embarked  \n",
        "0  22.000000      1      0         A/5 21171   7.2500   NaN        S  \n",
        "1  38.000000      1      0          PC 17599  71.2833   C85        C  \n",
        "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
        "3  35.000000      1      0            113803  53.1000  C123        S  \n",
        "4  35.000000      0      0            373450   8.0500   NaN        S  \n",
        "5  29.699118      0      0            330877   8.4583   NaN        Q  \n",
        "6  54.000000      0      0             17463  51.8625   E46        S  \n",
        "7   2.000000      3      1            349909  21.0750   NaN        S  \n",
        "8  27.000000      0      2            347742  11.1333   NaN        S  \n",
        "9  14.000000      1      0            237736  30.0708   NaN        C  "
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create three dummy variables using get_dummies\n",
      "pd.get_dummies(titanic_c.embarked, prefix='embarked').head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>embarked_C</th>\n",
        "      <th>embarked_Q</th>\n",
        "      <th>embarked_S</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "   embarked_C  embarked_Q  embarked_S\n",
        "0           0           0           1\n",
        "1           1           0           0\n",
        "2           0           0           1\n",
        "3           0           0           1\n",
        "4           0           0           1\n",
        "5           0           1           0\n",
        "6           0           0           1\n",
        "7           0           0           1\n",
        "8           0           0           1\n",
        "9           1           0           0"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
      "embarked_dummies = pd.get_dummies(titanic_c.embarked, prefix='embarked').iloc[:, 1:]\n",
      "\n",
      "# join the two dummy variable columns onto the original DataFrame\n",
      "titanic_c = titanic_c.join(embarked_dummies)\n",
      "\n",
      "# print the updated DataFrame\n",
      "titanic_c.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>survived</th>\n",
        "      <th>pclass</th>\n",
        "      <th>name</th>\n",
        "      <th>sex</th>\n",
        "      <th>age</th>\n",
        "      <th>sibsp</th>\n",
        "      <th>parch</th>\n",
        "      <th>ticket</th>\n",
        "      <th>fare</th>\n",
        "      <th>cabin</th>\n",
        "      <th>embarked</th>\n",
        "      <th>embarked_Q</th>\n",
        "      <th>embarked_S</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                           Braund, Mr. Owen Harris</td>\n",
        "      <td> 1</td>\n",
        "      <td> 22.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>        A/5 21171</td>\n",
        "      <td>  7.2500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 38.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>         PC 17599</td>\n",
        "      <td> 71.2833</td>\n",
        "      <td>  C85</td>\n",
        "      <td> C</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td>                            Heikkinen, Miss. Laina</td>\n",
        "      <td> 0</td>\n",
        "      <td> 26.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> STON/O2. 3101282</td>\n",
        "      <td>  7.9250</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td>      Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 35.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           113803</td>\n",
        "      <td> 53.1000</td>\n",
        "      <td> C123</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                          Allen, Mr. William Henry</td>\n",
        "      <td> 1</td>\n",
        "      <td> 35.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           373450</td>\n",
        "      <td>  8.0500</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                                  Moran, Mr. James</td>\n",
        "      <td> 1</td>\n",
        "      <td> 29.699118</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>           330877</td>\n",
        "      <td>  8.4583</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> Q</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td>                           McCarthy, Mr. Timothy J</td>\n",
        "      <td> 1</td>\n",
        "      <td> 54.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>            17463</td>\n",
        "      <td> 51.8625</td>\n",
        "      <td>  E46</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td>                    Palsson, Master. Gosta Leonard</td>\n",
        "      <td> 1</td>\n",
        "      <td>  2.000000</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>           349909</td>\n",
        "      <td> 21.0750</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 27.000000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 2</td>\n",
        "      <td>           347742</td>\n",
        "      <td> 11.1333</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> S</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td>               Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
        "      <td> 0</td>\n",
        "      <td> 14.000000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td>           237736</td>\n",
        "      <td> 30.0708</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> C</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "   survived  pclass                                               name  sex  \\\n",
        "0         0       3                            Braund, Mr. Owen Harris    1   \n",
        "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
        "2         1       3                             Heikkinen, Miss. Laina    0   \n",
        "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
        "4         0       3                           Allen, Mr. William Henry    1   \n",
        "5         0       3                                   Moran, Mr. James    1   \n",
        "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
        "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
        "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
        "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
        "\n",
        "         age  sibsp  parch            ticket     fare cabin embarked  \\\n",
        "0  22.000000      1      0         A/5 21171   7.2500   NaN        S   \n",
        "1  38.000000      1      0          PC 17599  71.2833   C85        C   \n",
        "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
        "3  35.000000      1      0            113803  53.1000  C123        S   \n",
        "4  35.000000      0      0            373450   8.0500   NaN        S   \n",
        "5  29.699118      0      0            330877   8.4583   NaN        Q   \n",
        "6  54.000000      0      0             17463  51.8625   E46        S   \n",
        "7   2.000000      3      1            349909  21.0750   NaN        S   \n",
        "8  27.000000      0      2            347742  11.1333   NaN        S   \n",
        "9  14.000000      1      0            237736  30.0708   NaN        C   \n",
        "\n",
        "   embarked_Q  embarked_S  \n",
        "0           0           1  \n",
        "1           0           0  \n",
        "2           0           1  \n",
        "3           0           1  \n",
        "4           0           1  \n",
        "5           1           0  \n",
        "6           0           1  \n",
        "7           0           1  \n",
        "8           0           1  \n",
        "9           0           0  "
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a list of feature columns\n",
      "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
      "\n",
      "# define X and y\n",
      "X = titanic_c[feature_cols]\n",
      "y = titanic_c.survived\n",
      "# fit a classification tree with max_depth=3 on all data\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
      "treeclf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=1, splitter='best')"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## How to create a Graphviz file\n",
      "# with open(\"15_titanic.dot\", 'wb') as f:\n",
      "#     f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the feature importances\n",
      "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>feature</th>\n",
        "      <th>importance</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>     pclass</td>\n",
        "      <td> 0.242664</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>        sex</td>\n",
        "      <td> 0.655584</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>        age</td>\n",
        "      <td> 0.064494</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> embarked_Q</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> embarked_S</td>\n",
        "      <td> 0.037258</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "      feature  importance\n",
        "0      pclass    0.242664\n",
        "1         sex    0.655584\n",
        "2         age    0.064494\n",
        "3  embarked_Q    0.000000\n",
        "4  embarked_S    0.037258"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "# Compute the confusion matrix\n",
      "conf = metrics.confusion_matrix(y, treeclf.predict(X))\n",
      "print conf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[524  25]\n",
        " [133 209]]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Independent / On your Own\n",
      "Like we had done early optimizing the model, here are a few things we should try to make sure we've fit our best model:\n",
      "1. Play with the stopping criteria in a loop and determine which produces the best cross validated model.\n",
      "2. Compare your best tree model to another classifier (KNN, logistic regression). Which seems to perform the best, and why do you think so?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Conclusion\n",
      "\n",
      "### Advantages of decision trees\n",
      "\n",
      "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
      "- Non-parametric (will do better than linear regression if relationship between predictors and response is highly non-linear)\n",
      "- Decision trees provide a clear indication of which fields are most important for prediction or classification.\n",
      "\n",
      "<img src=\"./img/linear_vs_tree.png\">\n",
      "\n",
      "### Disadvantages of decision trees:\n",
      "\n",
      "- Decision trees are prone to errors in classification problems with many class and relatively small number of training examples.\n",
      "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
      "- Can create biased trees if the classes are highly imbalanced\n",
      "- Decision tree can be computationally expensive to train. The process of growing a decision tree is computationally expensive. At each node, each candidate splitting field must be sorted before its best split can be found.\n",
      "\n",
      "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Additional Resources\n",
      "\n",
      "\n",
      "** Articles and Books **\n",
      "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)\n",
      "\n",
      "* [Basic Evaluation Measures for Classifier Performance](http://webdocs.cs.ualberta.ca/~eisner/measures.html)\n",
      "* [The Relationship Between Precision-Recall and ROC Curves](https://lirias.kuleuven.be/bitstream/123456789/295592/1/d.)\n",
      "* [Precision recall sensitivity and specificity](http://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/)\n",
      "* [Harvard CS109 Slides on Decision Trees](https://drive.google.com/drive/u/0/#folders/0B7IVstmtIvlHbnFKbDlmdFFyU2s)\n",
      "* Additional thoughts on [Gini Vs Entropy](http://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)\n",
      "\n",
      "- [Introduction to Data Mining (Ch.4)](http://www-users.cs.umn.edu/~kumar/dmbook/index.php)\n",
      "\n",
      "** ROC **\n",
      "- [Really Awesome Paper!](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf)\n",
      "\n",
      "**Installing Graphviz (optional):**\n",
      "* Mac:\n",
      "    * [Download and install PKG file](http://www.graphviz.org/Download_macos.php)\n",
      "* Windows:\n",
      "    * [Download and install MSI file](http://www.graphviz.org/Download_windows.php)\n",
      "    * Add it to your Path: Go to Control Panel, System, Advanced System Settings, Environment Variables. Under system variables, edit \"Path\" to include the path to the \"bin\" folder, such as: `C:\\Program Files (x86)\\Graphviz2.38\\bin`"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}