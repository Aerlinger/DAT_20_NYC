{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at the twenty news groups dataset, another common ML dataset for comparison. The dataset is called [\u201cTwenty Newsgroups\u201d.](http://qwone.com/~jason/20Newsgroups/) Here is the official description, quoted from the [website:](http://qwone.com/~jason/20Newsgroups/)\n",
      "\n",
      "> The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper \u201cNewsweeder: Learning to filter netnews,\u201d though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
      "\n",
      "To make things go a little faster, we're only going to work with 4 categories out of the possible 20."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "categories = ['alt.atheism', \n",
      "              'soc.religion.christian', \n",
      "              'comp.graphics', \n",
      "              'sci.med']\n",
      "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(twenty_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "sklearn.datasets.base.Bunch"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print twenty_train.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: sd345@city.ac.uk (Michael Collier)\n",
        "Subject: Converting images to HP LaserJet III?\n",
        "Nntp-Posting-Host: hampton\n",
        "Organization: The City University\n",
        "Lines: 14\n",
        "\n",
        "Does anyone know of a good way (standard PC application/PD utility) to\n",
        "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
        "do the same, converting to HPGL (HP plotter) files.\n",
        "\n",
        "Please email any response.\n",
        "\n",
        "Is this the correct group?\n",
        "\n",
        "Thanks in advance.  Michael.\n",
        "-- \n",
        "Michael Collier (Programmer)                 The Computer Unit,\n",
        "Email: M.P.Collier@uk.ac.city                The City University,\n",
        "Tel: 071 477-8000 x3769                      London,\n",
        "Fax: 071 477-8565                            EC1V 0HB.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twenty_train.target[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twenty_train.target_names[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[twenty_train.target_names[t] for t in twenty_train.target[:10]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['comp.graphics',\n",
        " 'comp.graphics',\n",
        " 'soc.religion.christian',\n",
        " 'soc.religion.christian',\n",
        " 'soc.religion.christian',\n",
        " 'soc.religion.christian',\n",
        " 'soc.religion.christian',\n",
        " 'sci.med',\n",
        " 'sci.med',\n",
        " 'sci.med']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use `CountVectorizer` to make a matrix of word counts, where each row is a document and each column is the word, the cell `matrix[document][word]` contains the count of word in document.\n",
      "\n",
      "We can expand on this by setting the ngram_range. This parameter allows us to set each column not only as a word, but possibly sequences of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vect = CountVectorizer(stop_words='english', lowercase=True, ngram_range=(1,2))\n",
      "X_train_counts = count_vect.fit_transform(twenty_train.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calling `count_vect.fit_transform` both modifies `count_vect` buy having it \"learn\" the vocabulary of the documents, *and* returns the count vectors.\n",
      "\n",
      "`count_vect.vocabulary_` is now a dictionary mapping terms to feature indices."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_vect.vocabulary_.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "[(u'3ds2scn', 9380),\n",
        " (u'say say', 210528),\n",
        " (u'dietary needs', 73458),\n",
        " (u'lines david', 143081),\n",
        " (u'work olney', 261997),\n",
        " (u'independent supporting', 123087),\n",
        " (u'lecture believe', 139468),\n",
        " (u'unit point', 248735),\n",
        " (u'woods', 261409),\n",
        " (u'pdb bnl', 176267)]"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_vect.get_feature_names()[10100:10120]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "[u'47 fri',\n",
        " u'47 gary',\n",
        " u'47 god',\n",
        " u'47 kalat',\n",
        " u'47 let',\n",
        " u'47 lsran6inn14a',\n",
        " u'47 new',\n",
        " u'47 newsreader',\n",
        " u'47 organization',\n",
        " u'47 pub',\n",
        " u'47 qualities',\n",
        " u'47 smoked',\n",
        " u'47 volume',\n",
        " u'47 west',\n",
        " u'471',\n",
        " u'471 3241',\n",
        " u'471 93430',\n",
        " u'472',\n",
        " u'472 2227',\n",
        " u'472 5527']"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train_counts[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 654)\t2\n",
        "  (0, 658)\t2\n",
        "  (0, 806)\t1\n",
        "  (0, 2465)\t1\n",
        "  (0, 2517)\t1\n",
        "  (0, 10128)\t2\n",
        "  (0, 10130)\t1\n",
        "  (0, 10131)\t1\n",
        "  (0, 12659)\t1\n",
        "  (0, 12662)\t1\n",
        "  (0, 12952)\t1\n",
        "  (0, 12953)\t1\n",
        "  (0, 15778)\t2\n",
        "  (0, 15781)\t1\n",
        "  (0, 15800)\t1\n",
        "  (0, 18797)\t1\n",
        "  (0, 18839)\t1\n",
        "  (0, 24515)\t1\n",
        "  (0, 24546)\t1\n",
        "  (0, 51373)\t4\n",
        "  (0, 51376)\t1\n",
        "  (0, 51386)\t1\n",
        "  (0, 51426)\t2\n",
        "  (0, 53768)\t3\n",
        "  (0, 53771)\t1\n",
        "  :\t:\n",
        "  (0, 225098)\t1\n",
        "  (0, 228669)\t1\n",
        "  (0, 228806)\t1\n",
        "  (0, 235474)\t1\n",
        "  (0, 235479)\t1\n",
        "  (0, 237187)\t1\n",
        "  (0, 237192)\t1\n",
        "  (0, 237306)\t1\n",
        "  (0, 237310)\t1\n",
        "  (0, 240428)\t1\n",
        "  (0, 240429)\t1\n",
        "  (0, 247154)\t2\n",
        "  (0, 247163)\t1\n",
        "  (0, 247254)\t1\n",
        "  (0, 248714)\t1\n",
        "  (0, 248723)\t1\n",
        "  (0, 248997)\t2\n",
        "  (0, 249128)\t1\n",
        "  (0, 249221)\t1\n",
        "  (0, 252145)\t1\n",
        "  (0, 252148)\t1\n",
        "  (0, 258004)\t1\n",
        "  (0, 258355)\t1\n",
        "  (0, 265101)\t1\n",
        "  (0, 265102)\t1\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print count_vect.get_feature_names()[53771]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "collier programmer\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we also use a TF-IDF representation, which stands for Term Frequency - Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the CountVectorizer features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish what Inverse Document Frequency, first let's define Document Frequency. This is the % of documents that a particular word appears in. For example, you could assume \"the\" appears in 100% of documents, while words like \"Syria\" would have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although frequently the log is also taken).\n",
      "\n",
      "Looking at our final term - Term Frequency * Inverse Document Frequency = Term Frequency / Document Frequency. The intuition is that words that have high weight are though that either appear alot in this document or appear in very few other documents (somehow unique to this document)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
      "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
      "X_train_tfidf.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "(2257, 266966)"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train_tfidf[0, 53771]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0891641866607\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train_counts[0, 53771]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests are some of the most widespread classifiers used. They are relatively simple to use (very few parameters to set and easy to avoid overfitting). The only parameter we are really worried about is the number of trees we want to create - `n_estimators` in sklearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train_tfidf.toarray(), twenty_train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.81673307  0.78324468  0.79787234]\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_model = RandomForestClassifier(n_estimators=10)\n",
      "print cross_val_score(rf_model, X_train_tfidf.toarray(), twenty_train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.86985392  0.84707447  0.81914894]\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we saw before, getting information on what features we are using in Python can be difficult. Each vectorizer has `get_feature_names` which we can use to tie back to our dataset.\n",
      "\n",
      "Random Forests (and Decision Trees) have a good way extracting what features are important. Unlike Logistic Regression, we don't have coeffcients that tell us relative impact, but we can keep track of what features give us the best splits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_model_fitted = RandomForestClassifier(n_estimators=10).fit(X_train_tfidf.toarray(), twenty_train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_features = sorted(zip(rf_model_fitted.feature_importances_, count_vect.get_feature_names()), reverse=True)[:10]\n",
      "print sorted_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0.020667976387789756, u'graphics'), (0.0096340870315677583, u'image'), (0.0091959859260409144, u'rutgers edu'), (0.0082517474774505949, u'shameful'), (0.0082010294965111086, u'christians'), (0.0070965668711587158, u'christ'), (0.0067718921638842255, u'nntp posting'), (0.0064872289018116491, u'caltech edu'), (0.0062834077000430257, u'apr'), (0.0062037249063523264, u'people')]\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#docs_new = [x[1] for x in sorted_features]\n",
      "docs_new = []\n",
      "docs_new.append('''My point is that you set up your views as the only way to believe.  Saying\n",
      "that all eveil in this world is caused by atheism is ridiculous and\n",
      "counterproductive to dialogue in this newsgroups.  I see in your posts a\n",
      "spirit of condemnation of the atheists in this newsgroup bacause they don'\n",
      "t believe exactly as you do.  If you're here to try to convert the atheists\n",
      "here, you're failing miserably.  Who wants to be in position of constantly\n",
      "defending themselves agaist insulting attacks, like you seem to like to do?!\n",
      "I'm sorry you're so blind that you didn't get the messgae in the quote,\n",
      "everyone else has seemed to.''')\n",
      "docs_new.append('''At the cost of repudiating the FAQ, I think too much is made of the\n",
      "strong vs weak atheism issue, although in the context of alt.atheism,\n",
      "where we're continually attacked on the basis that strong atheists\n",
      "\"believe\" in the non-existence of god, I think the separation is a\n",
      "valid one.''')\n",
      "docs_new.append('''Ok, god has the disclaimer, reserves the right to judge individual\n",
      "cases.  If we believe him to be loving, then we also believe him to be\n",
      "able to serve justice to all.  Don't worry if a Jew, or athiest is\n",
      "going to heaven or hell, for that is god to judge (although truly\n",
      "if you were concerned you could only worry abput those who refuse to\n",
      "believe/satisfy gods decrees) as much as keeping yourself straight.\n",
      "If you see something going on that is wrong, discuss it and explore it\n",
      "before making summary judgement.  People have enough free will to choose\n",
      "for themselves, so don't force choices on them, just inform them\n",
      "of what they're choices are.  God will take care of the rest in his justice.''')\n",
      "docs_new.append('''    Help!! I need code/package/whatever to take 3-D data and turn it into\n",
      "a wireframe surface with hidden lines removed. I'm using a DOS machine, and\n",
      "the code can be in ANSI C or C++, ANSI Fortran or Basic. The data I'm using\n",
      "forms a rectangular grid.\n",
      "   Please post your replies to the net so that others may benefit. IMHO, this\n",
      "is a general interest question.\n",
      "   Thank you!!!!!!''')\n",
      "docs_new.append('''   I was wondering if anyone knows where I can get more information about\n",
      "the graphics in the WingCommander series, and the RealSpace system they use.\n",
      "I think it's really awesome, and wouldn't mind being able to use similar\n",
      "features in programs.  Thanks in advance.''')\n",
      "docs_new.append('''Listen: thrush is a recognized clinical syndrome with definite\n",
      "characteristics.  If you have thrush, you have thrush, because you can\n",
      "see the lesions and do a culture and when you treat it, it generally\n",
      "responds well, if you're not otherwise immunocompromised.  Noring's\n",
      "anal-retentive idee fixe on having a fungal infection in his sinuses\n",
      "is not even in the same category here, nor are these walking neurasthenics\n",
      "who are convinced they have \"candida\" from reading a quack book.''')\n",
      "docs_new.append('''I am not sure where to post this message, please contact me if I'm way off\n",
      "the mark.\n",
      "On 19.3.93 my wife went to her General Practitioner (Doctor). He mentioned\n",
      "an article from a medical journal that is of great interest to us. He had\n",
      "read it in the previous three months but has been unable to find it again.\n",
      "The article was about Whiplash Injury/Cervical Pain. It mentions the use of\n",
      "a MRI (Magnetic Resonance Imagery) machine as a diagnostic tool and the work\n",
      "of a neurosurgeon who relived cervical pain.\n",
      "This article is most likely in an Australian medical journal. I very much\n",
      "want to obtain the name of the article, journal and author because the case\n",
      "matches my wife. We would very much appreciate anyone's help in this matter\n",
      "via email preferably.''')\n",
      "\n",
      "X_new_counts = count_vect.transform(docs_new)\n",
      "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
      "predicted = rf_model_fitted.predict(X_new_tfidf.toarray())\n",
      "for doc, category in zip(docs_new, predicted):\n",
      "    print ('%r \\n=> %s\\n' % (doc, twenty_train.target_names[category]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"My point is that you set up your views as the only way to believe.  Saying\\nthat all eveil in this world is caused by atheism is ridiculous and\\ncounterproductive to dialogue in this newsgroups.  I see in your posts a\\nspirit of condemnation of the atheists in this newsgroup bacause they don'\\nt believe exactly as you do.  If you're here to try to convert the atheists\\nhere, you're failing miserably.  Who wants to be in position of constantly\\ndefending themselves agaist insulting attacks, like you seem to like to do?!\\nI'm sorry you're so blind that you didn't get the messgae in the quote,\\neveryone else has seemed to.\" \n",
        "=> soc.religion.christian\n",
        "\n",
        "'At the cost of repudiating the FAQ, I think too much is made of the\\nstrong vs weak atheism issue, although in the context of alt.atheism,\\nwhere we\\'re continually attacked on the basis that strong atheists\\n\"believe\" in the non-existence of god, I think the separation is a\\nvalid one.' \n",
        "=> sci.med\n",
        "\n",
        "\"Ok, god has the disclaimer, reserves the right to judge individual\\ncases.  If we believe him to be loving, then we also believe him to be\\nable to serve justice to all.  Don't worry if a Jew, or athiest is\\ngoing to heaven or hell, for that is god to judge (although truly\\nif you were concerned you could only worry abput those who refuse to\\nbelieve/satisfy gods decrees) as much as keeping yourself straight.\\nIf you see something going on that is wrong, discuss it and explore it\\nbefore making summary judgement.  People have enough free will to choose\\nfor themselves, so don't force choices on them, just inform them\\nof what they're choices are.  God will take care of the rest in his justice.\" \n",
        "=> soc.religion.christian\n",
        "\n",
        "\"    Help!! I need code/package/whatever to take 3-D data and turn it into\\na wireframe surface with hidden lines removed. I'm using a DOS machine, and\\nthe code can be in ANSI C or C++, ANSI Fortran or Basic. The data I'm using\\nforms a rectangular grid.\\n   Please post your replies to the net so that others may benefit. IMHO, this\\nis a general interest question.\\n   Thank you!!!!!!\" \n",
        "=> comp.graphics\n",
        "\n",
        "\"   I was wondering if anyone knows where I can get more information about\\nthe graphics in the WingCommander series, and the RealSpace system they use.\\nI think it's really awesome, and wouldn't mind being able to use similar\\nfeatures in programs.  Thanks in advance.\" \n",
        "=> comp.graphics\n",
        "\n",
        "'Listen: thrush is a recognized clinical syndrome with definite\\ncharacteristics.  If you have thrush, you have thrush, because you can\\nsee the lesions and do a culture and when you treat it, it generally\\nresponds well, if you\\'re not otherwise immunocompromised.  Noring\\'s\\nanal-retentive idee fixe on having a fungal infection in his sinuses\\nis not even in the same category here, nor are these walking neurasthenics\\nwho are convinced they have \"candida\" from reading a quack book.' \n",
        "=> sci.med\n",
        "\n",
        "\"I am not sure where to post this message, please contact me if I'm way off\\nthe mark.\\nOn 19.3.93 my wife went to her General Practitioner (Doctor). He mentioned\\nan article from a medical journal that is of great interest to us. He had\\nread it in the previous three months but has been unable to find it again.\\nThe article was about Whiplash Injury/Cervical Pain. It mentions the use of\\na MRI (Magnetic Resonance Imagery) machine as a diagnostic tool and the work\\nof a neurosurgeon who relived cervical pain.\\nThis article is most likely in an Australian medical journal. I very much\\nwant to obtain the name of the article, journal and author because the case\\nmatches my wife. We would very much appreciate anyone's help in this matter\\nvia email preferably.\" \n",
        "=> sci.med\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "twenty_test = fetch_20newsgroups(subset='test',\n",
      "    categories=categories, shuffle=True, random_state=42)\n",
      "docs_test = twenty_test.data\n",
      "\n",
      "X_test_counts = count_vect.transform(docs_test)\n",
      "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
      "predicted = rf_model_fitted.predict(X_test_tfidf.toarray())\n",
      "np.mean(predicted == twenty_test.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "0.75166444740346205"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "print(metrics.classification_report(twenty_test.target, predicted,\n",
      "    target_names=twenty_test.target_names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                        precision    recall  f1-score   support\n",
        "\n",
        "           alt.atheism       0.88      0.63      0.74       319\n",
        "         comp.graphics       0.62      0.93      0.74       389\n",
        "               sci.med       0.85      0.56      0.67       396\n",
        "soc.religion.christian       0.80      0.86      0.83       398\n",
        "\n",
        "           avg / total       0.78      0.75      0.75      1502\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.confusion_matrix(twenty_test.target, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "array([[202,  39,  18,  60],\n",
        "       [  3, 362,  14,  10],\n",
        "       [ 16, 145, 221,  14],\n",
        "       [  9,  38,   7, 344]])"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}