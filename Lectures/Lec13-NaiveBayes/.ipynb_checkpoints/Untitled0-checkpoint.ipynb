{
 "metadata": {
  "name": "",
  "signature": "sha256:30111a59d1d4d7a55c09b758ce545365f7e6d78054b11e5509adc4e250839d41"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Lars Buitinck <L.J.Buitinck@uva.nl>\n",
      "# License: BSD 3 clause\n",
      "\n",
      "from __future__ import print_function\n",
      "from collections import defaultdict\n",
      "import re\n",
      "import sys\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction import DictVectorizer, FeatureHasher\n",
      "\n",
      "\n",
      "def n_nonzero_columns(X):\n",
      "    \"\"\"Returns the number of non-zero columns in a CSR matrix X.\"\"\"\n",
      "    return len(np.unique(X.nonzero()[1]))\n",
      "\n",
      "\n",
      "def tokens(doc):\n",
      "    \"\"\"Extract tokens from doc.\n",
      "\n",
      "    This uses a simple regex to break strings into tokens. For a more\n",
      "    principled approach, see CountVectorizer or TfidfVectorizer.\n",
      "    \"\"\"\n",
      "    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))\n",
      "\n",
      "\n",
      "def token_freqs(doc):\n",
      "    \"\"\"Extract a dict mapping tokens from doc to their frequencies.\"\"\"\n",
      "    freq = defaultdict(int)\n",
      "    for tok in tokens(doc):\n",
      "        freq[tok] += 1\n",
      "    return freq\n",
      "\n",
      "\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'comp.graphics',\n",
      "    'comp.sys.ibm.pc.hardware',\n",
      "    'misc.forsale',\n",
      "    'rec.autos',\n",
      "    'sci.space',\n",
      "    'talk.religion.misc',\n",
      "]\n",
      "# Uncomment the following line to use a larger set (11k+ documents)\n",
      "#categories = None\n",
      "\n",
      "print(__doc__)\n",
      "print(\"Usage: %s [n_features_for_hashing]\" % sys.argv[0])\n",
      "print(\"    The default number of features is 2**18.\")\n",
      "print()\n",
      "\n",
      "try:\n",
      "    n_features = int(sys.argv[1])\n",
      "except IndexError:\n",
      "    n_features = 2 ** 18\n",
      "except ValueError:\n",
      "    print(\"not a valid number of features: %r\" % sys.argv[1])\n",
      "    sys.exit(1)\n",
      "\n",
      "\n",
      "print(\"Loading 20 newsgroups training data\")\n",
      "raw_data = fetch_20newsgroups(subset='train', categories=categories).data\n",
      "data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6\n",
      "print(\"%d documents - %0.3fMB\" % (len(raw_data), data_size_mb))\n",
      "print()\n",
      "\n",
      "print(\"DictVectorizer\")\n",
      "t0 = time()\n",
      "vectorizer = DictVectorizer()\n",
      "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n",
      "duration = time() - t0\n",
      "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
      "print(\"Found %d unique terms\" % len(vectorizer.get_feature_names()))\n",
      "print()\n",
      "\n",
      "print(\"FeatureHasher on frequency dicts\")\n",
      "t0 = time()\n",
      "hasher = FeatureHasher(n_features=n_features)\n",
      "X = hasher.transform(token_freqs(d) for d in raw_data)\n",
      "duration = time() - t0\n",
      "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
      "print(\"Found %d unique terms\" % n_nonzero_columns(X))\n",
      "print()\n",
      "\n",
      "print(\"FeatureHasher on raw tokens\")\n",
      "t0 = time()\n",
      "hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
      "X = hasher.transform(tokens(d) for d in raw_data)\n",
      "duration = time() - t0\n",
      "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
      "print(\"Found %d unique terms\" % n_nonzero_columns(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Automatically created module for IPython interactive environment\n",
        "Usage: -c [n_features_for_hashing]\n",
        "    The default number of features is 2**18.\n",
        "\n",
        "not a valid number of features: '-f'\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "1",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}