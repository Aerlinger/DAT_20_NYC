{
 "metadata": {
  "name": "",
  "signature": "sha256:0fb701eb4feb92b69669da0d5a2bfca7bd0d26ba369bc695428034c483316779"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%html\n",
      "<link rel=\"stylesheet\" href=\"../..//hyrule.css\" type=\"text/css\">"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<link rel=\"stylesheet\" href=\"../..//hyrule.css\" type=\"text/css\">"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x11611a550>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lecture 13: Naive Bayes\n",
      "\n",
      "* Understanding the similarities and differences between Bayes Classifiers and Logistic Regression\n",
      "* Using a vectorizer to transform your text data\n",
      "* Learning the difference between a sparse and normal array\n",
      "* Identifying and interpreting traits of learners through comparison\n",
      "\n",
      "\n",
      "### Let's start with an illustrative example:\n",
      "\n",
      "Suppose there is a type of cancer which occurs in 1% of the population.\n",
      "\n",
      "There is a cancer with the following properties:\n",
      "- 90% chance test is **positive** when patient **has cancer**  *(what's this called?)*\n",
      "- 90% chance test is **negative** when patient does not **have cancer**\n",
      "\n",
      "**Q:** Does the patient actually have cancer?\n",
      "\n",
      "> Take a moment to write out the answer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Joint Probability:\n",
      "\n",
      "1. The probability a person **has cancer** and receives a **positive** test result is the product of the two events:\n",
      "\n",
      "    $\\Pr(C, +) = \\Pr(C) * \\Pr(+ \\mid C) = 0.009$\n",
      "\n",
      "\n",
      "2. Probability a person **does not have cancer** and receives a **negative** test result.\n",
      "\n",
      "    $\\Pr(\\neg C, -) = \\Pr(\\neg C) * \\Pr(- \\mid \\neg C) = 0.099$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Prior and Posterior Probabilities:\n",
      "\n",
      "####Prior Probability: \n",
      "*Probabilities known **before the outcome** of our test*\n",
      "\n",
      "1. Probability of a person having cancer. $\\Pr(C) = 0.01$\n",
      "\n",
      "2. Probability of a **positive** test result, given a person **has cancer**. $\\Pr(+\\mid C) = 0.9$\n",
      "\n",
      "3. Probability of a **negative** test result, given a person **does not have cancer.** $\\Pr(-\\mid C) = 0.9$\n",
      "\n",
      "\n",
      "####Posterior Probability:\n",
      "\n",
      "1. The probability a person **has cancer** given they receive a **positive** test result (*our original question*).\n",
      "\n",
      "    $\\Pr(C \\mid +) = ?$ **(A)**\n",
      "\n",
      "\n",
      "2. Probability a person **does not have cancer** given a **negative** test result.\n",
      "\n",
      "    $\\Pr(\\neg C \\mid -) = ?$ **(B)**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Deriving Bayes' Law:\n",
      "\n",
      "Let's think about what we need to solve **(A)**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "*Normalization Factor (Total probability):*\n",
      "\n",
      "$\\Pr(+) = \\Pr(C, +) + \\Pr(\\neg C, +)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### We now have all of the information to solve **(A)**:\n",
      "\n",
      "$\\Pr(C \\mid +) = \\dfrac{\\Pr(+ \\mid C)\\Pr(C)}{\\Pr(C, +) + \\Pr(\\neg C, +)} = \\dfrac{\\Pr(+ \\mid C)\\Pr(C)}{\\Pr(+)}$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More generally this can be expressed as:\n",
      "\n",
      "$\\Pr(H\\mid E) = \\dfrac{\\Pr(E\\mid H)\\Pr(H)}{\\Pr(E)}$\n",
      "\n",
      "Where $H$ is our hypothesis and $E$ is evidence for our hypothesis.\n",
      "\n",
      "**This formula is known as Bayes' Theorem**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using notation from previous classes we're trying to predict the probability of some target class variable $y$ (the hypothesis $H$) given a set of observations (evidence $E$) $X_{n}$. Thus, the previous equation can be rewritten as follows:\n",
      "\n",
      "$Pr(y \\mid X_{n}) = \\dfrac{Pr(X_{n} \\mid y)Pr(y)}{Pr(X_{n})}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What's the importance of the Naive Bayes Equation -- what do we need to know?\n",
      "\n",
      "#### Why Naive?\n",
      "It assumes all inputs (categories, predictors) are independent: In the case of text: its blind to word context and meaning: \n",
      "\n",
      "#### Naivete helps because negligible information is required\n",
      "Simply word counting and that's it -- faster testing and very simple construction on ANY text.  Naivete doesn't hurt because correctness is based on classification, not prediction.\n",
      "\n",
      "#### Good Predictor, (possibly) Bad Estimator\n",
      "NB chooses among possible categories to find the one that is the greatest.  The associated probability assigned is not necessarily accurate.  For example -- it might predict an email is Spam at 80%, but the true value might be 95% -- this doesn't matter, since its just choosing the category, anything over 50% will give it a perfect score. So while good estimation means good prediction,  good prediction doesn't necessarily mean good estimation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A great explanation of Bayes Classification\n",
      "*from [Mathias Brandewinder](http://clear-lines.com/blog/post/Naive-Bayes-Classification.aspx)*\n",
      "\n",
      "The canonical application of Bayes na\u00efve classification is in text classification, where the goal is to identify to which pre-determined category a piece of text belongs to  \u2013 for instance, is this email I just received spam, or ham (\u201cvaluable\u201d email)?\n",
      "\n",
      "The underlying idea is to use individual words present in the text as indications for what category it is most likely to belong to, using Bayes Theorem, named after the cheerful-looking Reverend Bayes.\n",
      "\n",
      "Imagine that you received an email containing the words \u201cNigeria\u201d, \u201cPrince\u201d, \u201cDiamonds\u201d and \u201cMoney\u201d. It is very likely that if you look into your spam folder, you\u2019ll find quite a few emails containing these words, whereas, unless you are in the business of importing diamonds from Nigeria and have some aristocratic family, your \u201cnormal\u201d emails would rarely contain these words. They have a much higher frequency within the category \u201cSpam\u201d than within the Ham, which makes them a potential flag for undesired business ventures.\n",
      "\n",
      "On the other hand, let\u2019s assume that you are a lucky person, and that typically, what you receive is Ham, with the occasional Spam bit. If you took a random email in your inbox, it is then much more likely that it belongs to the Ham category.\n",
      "\n",
      "Bayes\u2019 Theorem combines these two pieces of information together, to determine the probability that a particular email belongs to the \u201cSpam\u201d category, if it contains the word \u201cNigeria\u201d:\n",
      "\n",
      "$P(is Spam|contains Nigeria) = \\dfrac{P(contains Nigeria|is Spam) P(is Spam)}{P(contains Nigeria)}$\n",
      "\n",
      "In other words, 2 factors should be taken into account when deciding whether an email containing \u201cNigeria\u201d is spam: how over-represented is that word in Spam, and how likely is it that any email is spammy in the first place?\n",
      "\n",
      "The algorithm is named \u201cNa\u00efve\u201d, because it makes a simplifying assumption about the text, which turns out to be very convenient for computations purposes, namely that each word appears with a frequency which doesn\u2019t depend on other words. This is an unlikely assumption (the word \u201cDiamond\u201d is much more likely to be present in an email containing \u201cNigeria\u201d than in your typical family-members discussion email).\n",
      "\n",
      "We\u2019ll leave it at that on the concepts \u2013  I\u2019ll refer the reader who want to dig deeper to the book, or to this explanation of text classification with Na\u00efve Bayes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data Problem: Predicting Moving Ratings Given Text Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import cross_validation\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Vectorizing with SK-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "\n",
      "CountVectorizer?\n",
      "\n",
      "# Like the learners in sklearn, CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "\n",
      "# then, use `get_feature_names` to return the tokens\n",
      "print vectorizer.get_feature_names()\n",
      "\n",
      "# finally, call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'exciting', u'exciting exciting', u'exciting math', u'great', u'is', u'is great', u'is really', u'math', u'math is', u'really', u'really great']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####CountVectorizer Doc: \n",
      "\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Comparing a Sparse Matrix and Normal Matrix\n",
      "\n",
      "Vectorizers by default return back a sparse matrix. This is primarily because, as we'd expect, the majority of features will be 0, and at that point, it's more memory efficient to store the entry relationship and value. Pay attention to what we remember about matrix logic (how we refer to an entry) and how the toarray() function returns back the matrix we'd expect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Sparse Matrix'\n",
      "print x # A compressed version; the \"sparse\" matrix.\n",
      "print type(x) # type is from scipy, a library we've spoken very little of, but is pretty fantastic.\n",
      "print\n",
      "print 'Matrix'\n",
      "x_back = x.toarray()\n",
      "print x_back"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sparse Matrix\n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n",
        "<class 'scipy.sparse.csr.csr_matrix'>\n",
        "\n",
        "Matrix\n",
        "[[0 0 0 1 1 1 0 1 1 0 0]\n",
        " [0 0 0 1 1 0 1 1 1 1 1]\n",
        " [2 1 1 0 0 0 0 1 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we wanted the extra overhead, we could use the feature names as columns to a dataframe:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>exciting</th>\n",
        "      <th>exciting exciting</th>\n",
        "      <th>exciting math</th>\n",
        "      <th>great</th>\n",
        "      <th>is</th>\n",
        "      <th>is great</th>\n",
        "      <th>is really</th>\n",
        "      <th>math</th>\n",
        "      <th>math is</th>\n",
        "      <th>really</th>\n",
        "      <th>really great</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "   exciting  exciting exciting  exciting math  great  is  is great  is really  math  math is  really  really great\n",
        "0         0                  0              0      1   1         1          0     1        1       0             0\n",
        "1         0                  0              0      1   1         0          1     1        1       1             1\n",
        "2         2                  1              1      0   0         0          0     1        0       0             0"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preparing our Features $X$ and Target $y$ for Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics = pd.read_csv('../../data/rt_critics.csv')\n",
      "\n",
      "critics.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>        Derek Adams</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>       Time Out</td>\n",
        "      <td> So ingenious in concept, design and execution ...</td>\n",
        "      <td> 2009-10-04</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>    Richard Corliss</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>  TIME Magazine</td>\n",
        "      <td>                 The year's most inventive comedy.</td>\n",
        "      <td> 2008-08-31</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>        David Ansen</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>       Newsweek</td>\n",
        "      <td> A winning animated feature that has something ...</td>\n",
        "      <td> 2008-08-18</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>      Leonard Klady</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>        Variety</td>\n",
        "      <td> The film sports a provocative and appealing st...</td>\n",
        "      <td> 2008-06-09</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Jonathan Rosenbaum</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td> Chicago Reader</td>\n",
        "      <td> An entertaining computer-generated, hyperreali...</td>\n",
        "      <td> 2008-03-10</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
        "0         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
        "1     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
        "2         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
        "3       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
        "4  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that Kaggle was running a [competition](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) on accurately rating movies using this dataset.<br />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $X$ is a `(nreview, nwords)` array. Each row corresponds to a bag-of-words representation for a single review. This will be the *input* to the model.\n",
      "* $y$ is a `nreview`-element 1/0 array, encoding whether a review is Fresh (1) or Rotten (0). This is the desired *output* "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print critics.quote[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A winning animated feature that has something for everyone on the age spectrum.\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a vector where each row is bag-of-words for a single quote vectorizer is now going to forget the original feature set.\n",
      "What types of ngrams should we be getting given the settings we used when creating the vectorizer instance?\n",
      "\n",
      "What's the difference between `fit` and `fit_transform`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(stop_words=['in', 'the', 'of', 'and', 'is', 'a', 'for'])\n",
      "\n",
      "x = vectorizer.fit_transform(critics.quote)\n",
      "# Create an array where each element encodes whether the array is Fresh or Rotten\n",
      "y = (critics.fresh == 'fresh').values.astype(np.int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Creating the Classifier and Basic Accuracy\n",
      "\n",
      "We've talked in class before about how accuracy can be a very simple measure to evaluate a classifier model performance. Below, we have a function that evaluates the accuracy of a test train split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "def train_and_measure(classifier, x, y):\n",
      "    \"\"\"\n",
      "    Function accepts a classifer from sklearn and computes the accuracy measure for a random train and test split\n",
      "    classifier: an sklearn class\n",
      "    x         : a matrix of features\n",
      "    y         : a vector of targets\n",
      "    \"\"\"\n",
      "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=0.2, random_state=1234)\n",
      "    time_train_start = time.time()\n",
      "    clf = classifier.fit(xtrain, ytrain)\n",
      "    time_train_stop = time.time()\n",
      "\n",
      "    #Print the accuracy on the test and training dataset\n",
      "    time_score_start = time.time()\n",
      "    training_accuracy = clf.score(xtrain, ytrain)\n",
      "    test_accuracy = clf.score(xtest, ytest)\n",
      "    time_score_stop = time.time()\n",
      "    \n",
      "    print \"\\n-----------------------------------------------------------------------------\"\n",
      "    print classifier\n",
      "    print \"-----------------------------------------------------------------------------\"\n",
      "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
      "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
      "    print \"Training time: %0.4f\" % (time_train_stop - time_train_start)\n",
      "    print \"Scoring time: %0.4f\" % (time_score_stop - time_score_start)\n",
      "    print \"-----------------------------------------------------------------------------\"\n",
      "    \n",
      "\n",
      "\n",
      "train_and_measure(naive_bayes.MultinomialNB(), x, y)\n",
      "\n",
      "x_ones = (x > 1) # recall that a bernoulli interpretation will only work with 1s and 0s, or binary data.\n",
      "train_and_measure(naive_bayes.BernoulliNB(), x_ones, y)\n",
      "\n",
      "# and for the heck of it:\n",
      "\n",
      "from sklearn import linear_model\n",
      "train_and_measure(linear_model.LogisticRegression(), x, y)\n",
      "\n",
      "from sklearn import neighbors\n",
      "train_and_measure(neighbors.KNeighborsClassifier(), x, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "-----------------------------------------------------------------------------\n",
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "-----------------------------------------------------------------------------\n",
        "Accuracy on training data: 0.92\n",
        "Accuracy on test data:     0.77\n",
        "Training time: 0.0078\n",
        "Scoring time: 0.0049\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "-----------------------------------------------------------------------------\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "-----------------------------------------------------------------------------\n",
        "Accuracy on training data: 0.62\n",
        "Accuracy on test data:     0.59\n",
        "Training time: 0.0036\n",
        "Scoring time: 0.0037\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "-----------------------------------------------------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "-----------------------------------------------------------------------------\n",
        "Accuracy on training data: 0.97\n",
        "Accuracy on test data:     0.76\n",
        "Training time: 0.1475\n",
        "Scoring time: 0.0018\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "-----------------------------------------------------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
        "-----------------------------------------------------------------------------\n",
        "Accuracy on training data: 0.76\n",
        "Accuracy on test data:     0.61\n",
        "Training time: 0.0016\n",
        "Scoring time: 8.1506\n",
        "-----------------------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
      "--------------------------------------------------------\n",
      "`MultinomialNB` implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
      "\n",
      "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
      "\n",
      "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
      "\n",
      "where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
      "\n",
      "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing.\n",
      "\n",
      "### What's the difference between Multinomial and Bernoulli Naive Bayes?\n",
      "   * Try Bernoulli when your dictionary of words and document length is small.\n",
      "   * With Bernoulli, you may add in additional non-word features easily\n",
      "   * Multinomial usually performs better, takes into account frequency vs. presence\n",
      "   * Consider the probability for the prior of the term \"the\" -- Bernoulli: 100%; Multinomial 5%"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Your Turn: Writing a function that returns back the average and standard deviation of a k-fold cross validation\n",
      "\n",
      "We want a function that should do a very similar process as above, but now, to use k fold cross validation, and to return back two values: the mean and standard deviation accuracy from the k-fold train and test.\n",
      "\n",
      "Helper code below, use as you need. Steps to take:\n",
      "\n",
      "1. What are the arguments required in the function? What should be customizable?\n",
      "2. What code should never change?\n",
      "3. What should the return look like?\n",
      "\n",
      "**After:** Evaluate which model seems to do its best on the set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "kfold = cross_validation.KFold(n=x.shape[0], n_folds=5, shuffle=True, random_state=1234)\n",
      "\n",
      "train_acc = []\n",
      "test_acc = []\n",
      "for train_index, test_index in kfold:\n",
      "    clf = naive_bayes.MultinomialNB().fit(x[train_index], y[train_index])\n",
      "    train_acc.append(clf.score(x[train_index], y[train_index]))\n",
      "    test_acc.append(clf.score(x[test_index], y[test_index]))\n",
      "\n",
      "print np.array(test_acc).mean()\n",
      "print np.array(test_acc).std()\n",
      "\n",
      "plt.figure()\n",
      "plt.ylabel=\"train accuracy\"\n",
      "plt.plot(test_acc)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.779278815261\n",
        "0.00547288629026\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[<matplotlib.lines.Line2D at 0x116d540d0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD7CAYAAACIYvgKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7dqaUxI9gwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11611af10>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How do we evaluate the right K to use in K fold?\n",
      "\n",
      "It's very common to default to a typical value of K, but let's evaluate, for a minute, what happens to the accuracy of our training and test sets as K increases:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_train_acc = []\n",
      "k_test_acc = []\n",
      "for i in range(2, 20):\n",
      "    kfold = cross_validation.KFold(n=x.shape[0], n_folds=i, shuffle=True, random_state=1234)\n",
      "    test_acc, train_acc = [], []\n",
      "    for train_index, test_index in kfold:\n",
      "        clf = naive_bayes.MultinomialNB().fit(x[train_index], y[train_index])\n",
      "        train_acc.append(clf.score(x[train_index], y[train_index]))\n",
      "        test_acc.append(clf.score(x[test_index], y[test_index]))\n",
      "    k_train_acc.append(np.array(train_acc).mean())\n",
      "    k_test_acc.append(np.array(test_acc).mean())\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(range(2, 20), k_train_acc)\n",
      "plt.plot(range(2, 20), k_test_acc);\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAp7EZEA/H9G1HCLZCKNSwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x116d54050>"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above we call a fit chart. How does this change as we attempt to use other classifiers? (Keep in mind when switching to LogisticRegression how much **slower** it will be compared to Bayes!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reviewing Error: where did we go wrong?\n",
      "\n",
      "Below, let's pull out instances where the classifier did not classify the result correctly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Review errors\n",
      "prob = clf.predict_proba(x)[:, 0]\n",
      "bad_rotten = np.argsort(prob[y == 0])[:5]\n",
      "bad_fresh = np.argsort(prob[y == 1])[-5:]\n",
      "\n",
      "print \"Mis-predicted Rotten quotes\"\n",
      "print '---------------------------'\n",
      "for row in bad_rotten:\n",
      "    print critics[y == 0].quote.irow(row)\n",
      "    print\n",
      "\n",
      "print \"Mis-predicted Fresh quotes\"\n",
      "print '--------------------------'\n",
      "for row in bad_fresh:\n",
      "    print critics[y == 1].quote.irow(row)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mis-predicted Rotten quotes\n",
        "---------------------------\n",
        "Peter's Friends won't win over anyone looking for depth - as drama, it's pure popcorn - but the vignettes are swept along by Branagh's richly theatrical temperament and by the exuberant wit of the cast.\n",
        "\n",
        "As for the animation, computer technology invests contemporary features with sometimes breathtaking dynamism, but outback flora being what it is, this isn't the most colourful Disney movie.\n",
        "\n",
        "A complicated film that never really successfully yokes together the themes of money-making and sexuality, it reveals both Kazan's operatic sensibility and his inability to follow an argument rigorously through.\n",
        "\n",
        "Working from an Elmore Leonard novel, Tarantino has created a gangster fiction that is never larger than life and sometimes smaller.\n",
        "\n",
        "What if this lesser-known chapter of German resistance had been more deeply captured? What if the moral conflicts running through this movie about love of country and revolt said more about Germany, war and, yes, genocide?\n",
        "\n",
        "Mis-predicted Fresh quotes\n",
        "--------------------------\n",
        "Basically, the movie's an extended setup for a dinner-table comedy of errors, in which the mismatched relatives confront one another in a nerve-racking test of appearances.\n",
        "\n",
        "Some of the gags don't work, but fewer than in any previous Brooks film that I've seen, and when the jokes are meant to be bad, they are riotously poor. What more can one ask of Mel Brooks?\n",
        "\n",
        "There's too much talent and too strong a story to mess it up. There was potential for more here, but this incarnation is nothing to be ashamed of, and some of the actors answer the bell.\n",
        "\n",
        "Deja Vu is well worth seeing for its visual brio, particularly the boom-crash opera of the ferry explosion, and a chase scene in which Washington is dodging downtown traffic on two temporal planes simultaneously.\n",
        "\n",
        "Weighed down by a dull setup featuring Ralph 'Karate Kid' Macchio, the movie gets a much-needed charge from Pesci, a bundle of bandy-legged impudence as Macchio's lawyer cousin, Vincent Gambini.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"audacious in its\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>7658</th>\n",
        "      <td> Mick LaSalle</td>\n",
        "      <td> rotten</td>\n",
        "      <td> 386117</td>\n",
        "      <td> San Francisco Chronicle</td>\n",
        "      <td> Where the Wild Things Are is audacious in its ...</td>\n",
        "      <td> 2009-10-16</td>\n",
        "      <td> 770671948</td>\n",
        "      <td> Where the Wild Things Are</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "            critic   fresh    imdb              publication                                              quote review_date       rtid                      title\n",
        "7658  Mick LaSalle  rotten  386117  San Francisco Chronicle  Where the Wild Things Are is audacious in its ...  2009-10-16  770671948  Where the Wild Things Are"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"own payoff is\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>5965</th>\n",
        "      <td> Mike Clark</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 115710</td>\n",
        "      <td> USA Today</td>\n",
        "      <td> The movie's own payoff is compelling enough, b...</td>\n",
        "      <td> 2000-01-01</td>\n",
        "      <td> 364525542</td>\n",
        "      <td> Blood and Wine</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "          critic  fresh    imdb publication                                              quote review_date       rtid           title\n",
        "5965  Mike Clark  fresh  115710   USA Today  The movie's own payoff is compelling enough, b...  2000-01-01  364525542  Blood and Wine"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How might we improve the model?\n",
      "\n",
      "*Some words are more useful than others*\n",
      "\n",
      "**Tf-idf** stands for *Term Frequency-Inverse Document Frequency*, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.\n",
      "\n",
      "> The importance increases proportionally to the number of times a word appears in the *document* but is offset by the frequency of the word in the *corpus*.\n",
      "\n",
      "#### Definition:\n",
      "\n",
      "- **TF: Term Frequency** Measure of how *frequently a term occurs* in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
      "\n",
      "\n",
      "$TF(t)$ = (Number of times term $t$ appears in a document) / (Total number of terms in the document).\n",
      "\n",
      "\n",
      "- ** IDF: Inverse Document Frequency:** A measure of how *important* a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
      "\n",
      "$IDF(t)$ = $\\ln$(Total number of documents / Number of documents containing term $t$).\n",
      "\n",
      "\n",
      "## TF-IDF Example:\n",
      "\n",
      "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4.\n",
      "\n",
      "##### Term Frequency:\n",
      "Number of terms: 100\n",
      "\n",
      "Number of appearances of 'cat': 3\n",
      "\n",
      "\n",
      "$TF(t) = 3/100$\n",
      "\n",
      "\n",
      "##### Inverse Document Frequency (Importance/rarity):\n",
      "Number of documents: 10 Million\n",
      "\n",
      "Number of documents containing the word 'Cat': 1,000\n",
      "\n",
      "$\\ln(10,000,000 / 1,000) = 13.29$\n",
      "\n",
      "##### Tf-Idf\n",
      "Thus, the Tf-idf weight is the product of these quantities: \n",
      "\n",
      "$TfIdf(t) = 0.03 * 13.29 = 0.399$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert text features to TfIdf weights\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Convert sparse matrix to TfIdf weighted sparse matrix\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "tfidf_trans = TfidfTransformer()\n",
      "\n",
      "# What is x?\n",
      "x_idf = tfidf_trans.fit_transform(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_train_acc = []\n",
      "k_test_acc = []\n",
      "for i in range(2, 20):\n",
      "    kfold = cross_validation.KFold(n=x_idf.shape[0], n_folds=i, shuffle=True, random_state=1234)\n",
      "    test_acc, train_acc = [], []\n",
      "    for train_index, test_index in kfold:\n",
      "        clf = naive_bayes.MultinomialNB().fit(x_idf[train_index], y[train_index])\n",
      "        train_acc.append(clf.score(x_idf[train_index], y[train_index]))\n",
      "        test_acc.append(clf.score(x_idf[test_index], y[test_index]))\n",
      "    k_train_acc.append(np.array(train_acc).mean())\n",
      "    k_test_acc.append(np.array(test_acc).mean())\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(range(2, 20), k_train_acc)\n",
      "plt.plot(range(2, 20), k_test_acc);\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD7CAYAAACL+TRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1152d6ad0>"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vec = TfidfVectorizer()\n",
      "x_tfidf = tfidf_vec.fit_transform(critics.quote)\n",
      "\n",
      "k_train_acc = []\n",
      "k_test_acc = []\n",
      "for i in range(2, 20):\n",
      "    kfold = cross_validation.KFold(n=x_tfidf.shape[0], n_folds=i, shuffle=True, random_state=1234)\n",
      "    test_acc, train_acc = [], []\n",
      "    for train_index, test_index in kfold:\n",
      "        clf = naive_bayes.MultinomialNB().fit(x_tfidf[train_index], y[train_index])\n",
      "        train_acc.append(clf.score(x_tfidf[train_index], y[train_index]))\n",
      "        test_acc.append(clf.score(x_tfidf[test_index], y[test_index]))\n",
      "    k_train_acc.append(np.array(train_acc).mean())\n",
      "    k_test_acc.append(np.array(test_acc).mean())\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(range(2, 20), k_train_acc)\n",
      "plt.plot(range(2, 20), k_test_acc);\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9iIiMaCwFxGJAYW9iEgM/H8lV3Ted0tUpAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x113bdc6d0>"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### More interest in sentiment analysis?\n",
      "You can use the \"STS-Gold Sentiment Corpus\".  [CSV is here.](https://gist.githubusercontent.com/datadave/47ed59dd8733b2063dc6/raw/583615c70a1167fcd72899b2d2830493f1c616e6/sts_gold_tweet.csv)\n",
      "\n",
      "Or consider using this endpoint, where they'll send you back a sentiment score:\n",
      "```python\n",
      "url = 'http://www.datasciencetoolkit.org/text2sentiment/'\n",
      "\n",
      "# Loop through the sentences\n",
      "for sentence in sentences:\n",
      "    payload = {'text': sentence} # The sentence we want the sentiment of \n",
      "    headers = {'content-type': 'application/json'} # The type of data you are sending\n",
      "    r = requests.post(url, data=json.dumps(payload), headers=headers) # Send the data\n",
      "    print sentence, json.loads(r.text)['score'] # Print the results\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Additional Exercise:\n",
      "\n",
      "### How might we use Bayes' Theorem on the Weather Dataset?\n",
      "\n",
      "Consider the dataset from last class:\n",
      "\n",
      "<img src=\"img/weather_categorical.png\">\n",
      "\n",
      "What if we wanted to make a new prediction for `play` based on the following observation?\n",
      "\n",
      "<img src=\"img/weather_sample.png\">\n",
      "\n",
      "Using posterior probability, this is the same as saying: *\"for a given predicted value of `play` ('yes'/'no'), what is the probability that our input features are `Outlook = 'Sunny'`, `Temperature = 'cool'`, `Humidity = 'high'`, and `Windy = True`?\"*\n",
      "\n",
      "\n",
      "\n",
      "Let's compute the answer using Bayes Theorem:\n",
      "\n",
      "$Pr(yes \\mid X_{n}) = \\dfrac{Pr(X_{n} \\mid yes)Pr(yes)}{Pr(X_{n})} = ?$\n",
      "\n",
      "$Pr(no \\mid X_{n}) = \\dfrac{Pr(X_{n} \\mid no)Pr(no)}{Pr(X_{n})} = ?$\n",
      "\n",
      "where $X_{n}$ = [`'Sunny'`, `'cool'`, `'high'`, `True`] for each of our four respective features. The above probabilities are complementary $Pr(yes \\mid X_{n}) = 1 - Pr(no \\mid X_{n})$ and the larger of the two probabilities will be our predicted class.\n",
      "\n",
      "Let's break each factor into a cross-tabulation:\n",
      "\n",
      "<img src=\"img/weather_crosstab.png\">\n",
      "\n",
      "Let's compute each of the terms for $Pr(yes \\mid X_{n})$ using Bayes' theorem:\n",
      "\n",
      "$\\Pr(yes) = 9/14$   *(this is called the prior probability of our hypothesis, which is our guessing probability if we knew nothing about $X_{n}$)*\n",
      "\n",
      "$ \\Pr(X_{n} \\mid yes) = \\Pr(sunny \\mid yes) * \\Pr(cool \\mid yes) * \\Pr(high \\mid yes) * \\Pr(True \\mid yes) = 54 / 6561$\n",
      "\n",
      "We don't need to compute the denominator of Bayes Theorem since $\\Pr(yes \\mid X_{n}) = 1 - Pr(no \\mid X_{n})$.\n",
      "\n",
      "### On your own:\n",
      "\n",
      "1. Finish the above calculations to compute $Pr(yes \\mid X_{n})$ and  $Pr(no \\mid X_{n})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Additional Resources and Next Steps\n",
      "* [Visualizing Bayes' theorem](http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/)\n",
      "* [SKL Naive Bayes Documentation](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
      "* [Stanford Naive Bayes Math](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)\n",
      "* For an alternative introduction to Bayes' Theorem, [Bayes' Rule for Ducks](https://planspacedotorg.wordpress.com/2014/02/23/bayes-rule-for-ducks/), this [5-minute video on conditional probability](https://www.youtube.com/watch?v=Zxm4Xxvzohk), or these [slides on conditional probability](https://docs.google.com/presentation/d/1psUIyig6OxHQngGEHr3TMkCvhdLInnKnclQoNUr4G4U/edit#slide=id.gfc69f484_00) may be helpful.\n",
      "* For more details on Naive Bayes classification, Wikipedia has two useful articles ([Naive Bayes classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) and [Naive Bayes spam filtering](http://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)), and Cross Validated has an excellent [Q&A](http://stats.stackexchange.com/questions/21822/understanding-naive-bayes).\n",
      "* If you enjoyed Paul Graham's article, you can read [his follow-up article](http://www.paulgraham.com/better.html) on how he improved his spam filter and this [related paper](http://www.merl.com/publications/docs/TR2004-091.pdf) about state-of-the-art spam filtering in 2004.\n",
      "* [TfIdf tutorial](http://www.tfidf.com/)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}