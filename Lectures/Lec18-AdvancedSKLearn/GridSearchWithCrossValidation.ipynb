{
 "metadata": {
  "name": "",
  "signature": "sha256:6677e957edba3aeddfc22ba604ba4cca79d742c46612e4f360362f7783de2e8a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TASK: Searching for optimal parameters\n",
      "#\n",
      "# FUNCTION: GridSearchCV\n",
      "#\n",
      "# DOCUMENTATION: http://scikit-learn.org/stable/modules/grid_search.html\n",
      "# DATA: Titanic (n=891, p=5 selected, type=classification)\n",
      "# DATA DICTIONARY: https://www.kaggle.com/c/titanic-gettingStarted/data\n",
      "\n",
      "# read in and prepare titanic data\n",
      "import pandas as pd\n",
      "\n",
      "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/titanic.csv')\n",
      "titanic['sex'] = titanic.sex.map({\n",
      "  'female': 0,\n",
      "  'male': 1\n",
      "})\n",
      "\n",
      "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
      "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
      "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
      "\n",
      "# define X and y\n",
      "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
      "X = titanic[feature_cols]\n",
      "y = titanic.survived\n",
      "\n",
      "# use cross-validation to find best max_depth\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# try max_depth=2\n",
      "treeclf = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
      "cross_val_score(treeclf, X, y, cv=10, scoring='roc_auc').mean()\n",
      "\n",
      "# try max_depth=3\n",
      "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
      "cross_val_score(treeclf, X, y, cv=10, scoring='roc_auc').mean()\n",
      "\n",
      "# use GridSearchCV to automate the search\n",
      "\n",
      "treeclf = DecisionTreeClassifier(random_state=1)\n",
      "max_depth_range = range(1, 21)\n",
      "param_grid = dict(max_depth=max_depth_range)\n",
      "grid = GridSearchCV(treeclf, param_grid, cv=10, scoring='roc_auc')\n",
      "grid.fit(X, y)\n",
      "\n",
      "# check the results of the grid search\n",
      "grid.grid_scores_\n",
      "grid_mean_scores = [result[1] for result in grid.grid_scores_]\n",
      "grid_mean_scores\n",
      "\n",
      "# plot the results\n",
      "import matplotlib.pyplot as plt\n",
      "plt.plot(max_depth_range, grid_mean_scores)\n",
      "\n",
      "# what was best?\n",
      "grid.best_score_\n",
      "grid.best_params_\n",
      "grid.best_estimator_\n",
      "\n",
      "# search a \"grid\" of parameters\n",
      "max_depth_range = range(1, 21)\n",
      "min_samples_leaf_range = range(1, 11)\n",
      "param_grid = dict(max_depth=max_depth_range, min_samples_leaf=min_samples_leaf_range)\n",
      "grid = GridSearchCV(treeclf, param_grid, cv=10, scoring='roc_auc')\n",
      "grid.fit(X, y)\n",
      "grid.best_score_\n",
      "grid.best_params_\n",
      "\n",
      "\n",
      "## TASK: Standardization of features (aka \"center and scale\" or \"z-score normalization\")\n",
      "## FUNCTION: StandardScaler\n",
      "## DOCUMENTATION: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
      "## EXAMPLE: http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/preprocessing/about_standardization_normalization.ipynb\n",
      "## DATA: Wine (n=178, p=2 selected, type=classification)\n",
      "## DATA DICTIONARY: http://archive.ics.uci.edu/ml/datasets/Wine\n",
      "\n",
      "# sample data\n",
      "train = pd.DataFrame({'A':[40,50,60], 'B':[0.90,0.30,0.60], 'C':[0,0.20,0.80], 'label':[0,1,2]})\n",
      "oos = pd.DataFrame({'A':[54.9], 'B':[0.59], 'C':[0.79]})\n",
      "\n",
      "# define X and y\n",
      "X = train[['A','B','C']]\n",
      "y = train.label\n",
      "\n",
      "# KNN with k=1\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X, y)\n",
      "\n",
      "# what \"should\" it predict? what does it predict?\n",
      "knn.predict(oos)\n",
      "\n",
      "# standardize the features\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "\n",
      "# compare original to standardized\n",
      "X.values\n",
      "X_scaled\n",
      "\n",
      "# figure out how it standardized\n",
      "scaler.mean_\n",
      "scaler.std_\n",
      "(X.values-scaler.mean_) / scaler.std_\n",
      "\n",
      "# try this on real data\n",
      "wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None, usecols=[0,10,13])\n",
      "wine.columns=['label', 'color', 'proline']\n",
      "wine.head()\n",
      "wine.describe()\n",
      "\n",
      "# define X and y\n",
      "X = wine[['color', 'proline']]\n",
      "y = wine.label\n",
      "\n",
      "# split into train/test\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "# standardize\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_train)\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "\n",
      "# check that it worked properly\n",
      "X_train_scaled[:, 0].mean()\n",
      "X_train_scaled[:, 0].std()\n",
      "X_train_scaled[:, 1].mean()\n",
      "X_train_scaled[:, 1].std()\n",
      "\n",
      "# standardize X_test\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "\n",
      "# is this right?\n",
      "X_test_scaled[:, 0].mean()\n",
      "X_test_scaled[:, 0].std()\n",
      "X_test_scaled[:, 1].mean()\n",
      "X_test_scaled[:, 1].std()\n",
      "\n",
      "# compare KNN accuracy on original vs scaled data\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X_train, y_train)\n",
      "knn.score(X_test, y_test)\n",
      "knn.fit(X_train_scaled, y_train)\n",
      "knn.score(X_test_scaled, y_test)\n",
      "\n",
      "\n",
      "## TASK: Chaining steps\n",
      "## FUNCTION: Pipeline\n",
      "## DOCUMENTATION: http://scikit-learn.org/stable/modules/pipeline.html\n",
      "## DATA: Wine (n=178, p=2 selected, type=classification)\n",
      "## DATA DICTIONARY: http://archive.ics.uci.edu/ml/datasets/Wine\n",
      "\n",
      "# here is proper cross-validation on the original (unscaled) data\n",
      "X = wine[['color', 'proline']]\n",
      "y = wine.label\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "cross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()\n",
      "\n",
      "# why is this improper cross-validation on the scaled data?\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler.fit_transform(X)\n",
      "cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy').mean()\n",
      "\n",
      "# fix this using Pipeline\n",
      "from sklearn.pipeline import make_pipeline\n",
      "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
      "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n",
      "\n",
      "# using GridSearchCV with Pipeline\n",
      "n_neighbors_range = range(1, 21)\n",
      "param_grid = dict(kneighborsclassifier__n_neighbors=n_neighbors_range)\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
      "grid.fit(X, y)\n",
      "grid.best_score_\n",
      "grid.best_params_\n",
      "\n",
      "\n",
      "## TASK: Regularized regression\n",
      "## FUNCTIONS: Ridge, RidgeCV, Lasso, LassoCV\n",
      "## DOCUMENTATION: http://scikit-learn.org/stable/modules/linear_model.html\n",
      "## DATA: Crime (n=319 non-null, p=122, type=regression)\n",
      "## DATA DICTIONARY: http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime\n",
      "\n",
      "# read in data, remove categorical features, remove rows with missing values\n",
      "crime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\n",
      "crime = crime.iloc[:, 5:]\n",
      "crime.dropna(inplace=True)\n",
      "\n",
      "# define X and y\n",
      "X = crime.iloc[:, :-1]\n",
      "y = crime.iloc[:, -1]\n",
      "\n",
      "# split into train/test\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "# linear regression\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lm = LinearRegression()\n",
      "lm.fit(X_train, y_train)\n",
      "lm.coef_\n",
      "\n",
      "# make predictions and evaluate\n",
      "import numpy as np\n",
      "from sklearn import metrics\n",
      "preds = lm.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "# ridge regression (alpha must be positive, larger means more regularization)\n",
      "from sklearn.linear_model import Ridge\n",
      "rreg = Ridge(alpha=0.1, normalize=True)\n",
      "rreg.fit(X_train, y_train)\n",
      "rreg.coef_\n",
      "preds = rreg.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "# use RidgeCV to select best alpha\n",
      "from sklearn.linear_model import RidgeCV\n",
      "alpha_range = 10.**np.arange(-2, 3)\n",
      "rregcv = RidgeCV(normalize=True, scoring='mean_squared_error', alphas=alpha_range)\n",
      "rregcv.fit(X_train, y_train)\n",
      "rregcv.alpha_\n",
      "preds = rregcv.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "# lasso (alpha must be positive, larger means more regularization)\n",
      "from sklearn.linear_model import Lasso\n",
      "las = Lasso(alpha=0.01, normalize=True)\n",
      "las.fit(X_train, y_train)\n",
      "las.coef_\n",
      "preds = las.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "# try a smaller alpha\n",
      "las = Lasso(alpha=0.0001, normalize=True)\n",
      "las.fit(X_train, y_train)\n",
      "las.coef_\n",
      "preds = las.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "# use LassoCV to select best alpha (tries 100 alphas by default)\n",
      "from sklearn.linear_model import LassoCV\n",
      "lascv = LassoCV(normalize=True)\n",
      "lascv.fit(X_train, y_train)\n",
      "lascv.alpha_\n",
      "lascv.coef_\n",
      "preds = lascv.predict(X_test)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
      "\n",
      "\n",
      "## TASK: Regularized classification\n",
      "## FUNCTION: LogisticRegression\n",
      "## DOCUMENTATION: http://scikit-learn.org/stable/modules/linear_model.html\n",
      "## DATA: Titanic (n=891, p=5 selected, type=classification)\n",
      "## DATA DICTIONARY: https://www.kaggle.com/c/titanic-gettingStarted/data\n",
      "\n",
      "# define X and y\n",
      "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
      "X = titanic[feature_cols]\n",
      "y = titanic.survived\n",
      "\n",
      "# split into train/test\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "# logistic regression\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logreg = LogisticRegression()\n",
      "logreg.fit(X_train, y_train)\n",
      "logreg.coef_\n",
      "\n",
      "# logistic regression with L2 penalty (C must be positive, smaller means more regularization)\n",
      "logreg = LogisticRegression(C=0.5, penalty='l2')\n",
      "logreg.fit(X_train, y_train)\n",
      "logreg.coef_\n",
      "\n",
      "# pipeline with scaling to select best C and penalty\n",
      "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
      "C_range = 10.**np.arange(-2, 3)\n",
      "penalty_options = ['l1', 'l2']\n",
      "param_grid = dict(logisticregression__C=C_range, logisticregression__penalty=penalty_options)\n",
      "grid = GridSearchCV(pipe, param_grid, cv=10, scoring='roc_auc')\n",
      "grid.fit(X, y)\n",
      "grid.best_score_\n",
      "grid.best_params_\n",
      "\n",
      "\n",
      "## TASK: Feature selection\n",
      "## FUNCTIONS: RFE, RFECV\n",
      "## DOCUMENTATION: http://scikit-learn.org/stable/modules/feature_selection.html\n",
      "## DATA: Crime (n=319 non-null, p=122, type=regression)\n",
      "## DATA DICTIONARY: http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime\n",
      "\n",
      "# define X and y\n",
      "X = crime.iloc[:, :-1]\n",
      "y = crime.iloc[:, -1]\n",
      "\n",
      "# split into train/test\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "# select \"best\" features (half of them by default)\n",
      "lm = LinearRegression()\n",
      "from sklearn.feature_selection import RFE\n",
      "selector = RFE(lm)\n",
      "selector.fit(X_train, y_train)\n",
      "selector.n_features_\n",
      "selector.support_\n",
      "selector.ranking_\n",
      "\n",
      "# let RFECV select the \"optimal\" number of features\n",
      "from sklearn.feature_selection import RFECV\n",
      "selector = RFECV(lm, cv=3, scoring='mean_squared_error')\n",
      "selector.fit(X, y)\n",
      "selector.n_features_\n",
      "selector.support_\n",
      "selector.ranking_\n",
      "\n",
      "# *tentative* advice for usage:\n",
      "# 1. scale features, then use RFECV to select the number of features (p)\n",
      "# 2. build pipeline: feature scaling, select p features using RFE, model\n",
      "# 3. GridSearchCV to select optimal parameters\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([  2,  11,  89,  93,  81,  40,  62,  17,  61,   1,   3,  65,  16,\n",
        "        10,  38,   1,   1,  60,  83,  15,  42,  43,  56,  99,  51,  72,\n",
        "        84,  12,  30,  71,   1,  32,  49,  48,  47,  87,  21,  22,  25,\n",
        "        85,  24,  23,  50,  29,   1,  34,  18,  52,  70,  75,  80,  27,\n",
        "         8,   1,   9,   1,   7,  79,   1,   1,  39,  31,  13,  14,  68,\n",
        "         1,   1,   1,  20,   6,  94,  73,  74,   1, 102,  96, 104,  98,\n",
        "        28,  44,  19,  45,   4,  53,  86,   5,  26,  41,  82,  90,  33,\n",
        "         1,  59,  78,  95,  58,   1,   1,   1,  64,  55,  37, 103,   1,\n",
        "        57, 100,  76,  67,  77,  69,  36,  92,  91,  63,  97,  54,  46,\n",
        "         1,  66, 101,  88,  35])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}