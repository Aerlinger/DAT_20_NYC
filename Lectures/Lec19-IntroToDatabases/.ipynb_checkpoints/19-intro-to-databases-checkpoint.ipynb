{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": "",
  "signature": "sha256:4a16e3b931cea586c878307bd42ed84c7ce21d73f387dd5d47d4c642a65a42f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Relational Databases and SQL"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Review the context of denormalized vs normalized data in relational databases\n",
      "* Compare and contrast SQL syntax to pandas (and when we should use what)\n",
      "* Gain insight behind advanced database useage and defined functions in SQL."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What are databases?\n",
      "\n",
      "Databases are a structured data source optimized for efficient retrieval and storage.\n",
      "\n",
      "* **structured**: we have to pre-define organization strategy\n",
      "* **retrieval**: the ability to read data out\n",
      "* **storage**: the ability to write data and save it"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What is a relational database?\n",
      "\n",
      "A database is essentially a collection of **tables** which represent individual entities or objects. \n",
      "\n",
      "* A table has many **columns** each of which has a name and type such as `INT`, `VARCHAR` (string), `BOOLEAN`, etc. Columns also have constraints to ensure consistency and structure in the table if necessary. For instance, a constraint might specify that all values in column are unique, non-null and so on. \n",
      "\n",
      "* **Rows** of the table make up the individual records of data. Sound familiar? Dataframes in pandas were inspired by SQL tables and many pandas operations we've covered have equivalent operations in SQL.\n",
      "\n",
      "* The **schema** is the definition, or blueprint, of the database. Fundamentally, it is a manifest containing the specification  for all tables and their respective column names, types, and indexes. **The schema is what gives a relational database its structure**\n",
      "\n",
      "### An example of a schema\n",
      "Consider a database for a website like Twitter. A `user` on Twitter can have many `tweets` and a `tweet` can have many `comments`. In such a **schema** we might have a table for `users`, a table for `tweets`, and table for `comments`.\n",
      "\n",
      "Each table should have a primary key column:  a unique identifier for that row. Additionally, each table _can_ have one or more **foreign key** column: an id that links this to another table. \n",
      "\n",
      "For instance, the `tweets` table might have a foreign key of `user_id` to reference the author (a `tweet` \"belongs to\" a `user`). Similarly, a comment might have a foreign key of `tweet_id` (a `comment` belongs to a `tweet` and by extension, also belongs to a `user`). A `user` also has many relationships which \"join\" that user to his or her followers or followees.\n",
      "\n",
      "<img src='assets/normalized.png' style='height: 50%; margin-left: 0;' />\n",
      "\n",
      "### Normalized schema\n",
      "In a **normalized** schema, tables are designed to be thin in order to minimize:\n",
      "\n",
      "1. The amount of repeated information\n",
      "2. The amount of bytes stored (Fewer columns per table!)\n",
      "3. Burden of maintaining **referential integrity** of our database. (What if the foreign key linking two tables gets lost?)\n",
      "\n",
      "*A high degree of independence and separation of the tables, but often many connections between them via foreign keys*\n",
      "\n",
      "**Key concept:** Redundancy is minimized at a cost of being spread over a larger number of tables. In order lookup specific information (such as finding a specific comment on a Tweet by a user) we have to write a more complicated query to join many tables to access information stored independently across each of them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='assets/erd.png' style='height: 75%; margin-left: 0;' />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Case in point; here is a relational diagram of a typical ecommerce platform_\n",
      "\n",
      "### Denormalized schema\n",
      "\n",
      "What if we had designed the database to look this way with one table?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='assets/denormalized.png' style='height: 75%; margin-left: 0;' />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Repeated information is increased; the user information is repeated in each row.\n",
      "2. There is increased text storage (text bytes are larger than integer bytes)\n",
      "3. There is no need to join!\n",
      "\n",
      "The tradeoff between normalized and denormalized data is **speed vs storage**. Storage (for the most part) is the same everywhere.. so let's focus on the speed side. Speed breaks down into _read speed_ and _write speed_.\n",
      "\n",
      "\n",
      "### Critical question \n",
      "Of the two data views:\n",
      "\n",
      "1. Which would we believe to be slower to *read* but faster to *write*? Why?\n",
      "2. Which would we believe to be slower to *write* but faster to *read*? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SQL syntax\n",
      "\n",
      "SQL (structured query language) is a query language for loading, retrieving, and updating data in relational databases. Most commonly used SQL databases include:\n",
      "\n",
      "1. Oracle and MySQL\n",
      "2. SQL Server\n",
      "3. PostgreSQL\n",
      "\n",
      "The SQL-like structure is also heavily borrowed in large scale data languages and platforms:\n",
      "\n",
      "1. Apache Hive\n",
      "2. Apache Drill (based on Google's Dremel)\n",
      "3. Spark SQL\n",
      "\n",
      "So it is important to learn the basics that fit across all platforms!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Good syntax\n",
      "\n",
      "While companies and data teams end up developing their own sense of SQL style, those new to SQL should adopt at least the following style:\n",
      "\n",
      "1. Keywords are upper case and begin new lines\n",
      "2. fields in their own lines\n",
      "3. continuations are indented\n",
      "\n",
      "This will be explained as we go through examples below. To help make some connections, there will be some python code blocks using pandas syntax to do similar statements to the SQL queries. They'll be labeled ***pandas*** and ***end_pandas*** to clarify where those are."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### SELECT\n",
      "Basic use case for pulling data from the database.\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    col1,\n",
      "    col2\n",
      "FROM table\n",
      "WHERE [some condition];\n",
      "```\n",
      "\n",
      "Example\n",
      "```SQL\n",
      "SELECT\n",
      "    poll_title,\n",
      "    poll_date\n",
      "FROM polls\n",
      "WHERE romney_pct > obama_pct;\n",
      "```\n",
      "\n",
      "***pandas***\n",
      "```python\n",
      "polls[polls.romney_pct > polls.obama_pct][['poll_title', 'poll_date']]\n",
      "```\n",
      "***end_pandas***\n",
      "\n",
      "\n",
      "Notes:\n",
      "\n",
      "1. The WHERE is optional, though ultimately filtering data is usually the point of querying from a database.\n",
      "2. You may SELECT as many columns as you'd like, and alias each."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Aggregations and GROUP BY\n",
      "In this SELECT style, columns are either group by keys, or aggregations. \n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    col1,\n",
      "    AVG(col2)\n",
      "FROM table\n",
      "GROUP BY col1;\n",
      "```\n",
      "\n",
      "Example\n",
      "```SQL\n",
      "SELECT\n",
      "    poll_date,\n",
      "    AVG(obama_pct)\n",
      "FROM polls\n",
      "GROUP BY poll_date;\n",
      "```\n",
      "\n",
      "***pandas***\n",
      "```python\n",
      "polls.groupby('poll_date').obama_pct.mean()\n",
      "```\n",
      "***end_pandas***\n",
      "\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "1. You may groupby and aggregate as many columns as you'd like.\n",
      "2. Fields that do NOT use aggregations must be in the group by. Some SQL databases will throw errors; others will give you the wrong data.\n",
      "3. Standard aggregations include `STDDEV, MIN, MAX, COUNT, SUM`; mostly aggregations that can be quickly solved. For example, `MEDIAN` is less often a function, as the solution is more complicated in SQL.\n",
      "\n",
      "**Questions:**\n",
      "\n",
      "1. Imagine a field of poll_state. How would we find the max obama_pct and romney_pct for each state?\n",
      "2. How would we return a count of polls by state and date?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### JOINs \n",
      "JOIN is widely used in normalized data in order for us to denormalize the information. Analysts who work in strong relational databases often have half a dozen joins in their queries.\n",
      "\n",
      "```SQL\n",
      "SELECT ...\n",
      "FROM orders\n",
      "INNER JOIN order_amounts a on a.order_id = orders.id\n",
      "INNER JOIN order_items i on i.order_id = orders.id\n",
      "INNER JOIN variants v on v.id = i.variant_id\n",
      "INNER JOIN products p on p.id = v.product_id\n",
      "INNER JOIN suppliers s on s.id = v.supplier_id\n",
      "INNER JOIN addresses ad on ad.addressable_type = 'Supplier' and ad.addressable_id = s.id\n",
      "...;\n",
      "```\n",
      "\n",
      "Basic Example:\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    t1.c1,\n",
      "    t1.c2,\n",
      "    t2.c2\n",
      "FROM t1 \n",
      "INNER JOIN t2 ON t1.c1 = t2.c2;\n",
      "```\n",
      "\n",
      "***pandas***\n",
      "```python\n",
      "t1.join(t2, on='c2')\n",
      "```\n",
      "***end_pandas***\n",
      "\n",
      "\n",
      "There are several join types used, despite the above only using one: `INNER JOIN`.\n",
      "\n",
      "<img src='assets/sql_join_venn.jpg' />\n",
      "\n",
      "Note that using JOIN introduces potential changes in our data context: **One to Many** and **Many to Many** relationships.\n",
      "\n",
      "\n",
      "<img src='assets/relationships.png'  style='width: 50%;'/>\n",
      "\n",
      "###Question:\n",
      "Remember our Twitter example? Which of those joins was *one-to-many* vs. *many-to-many*?\n",
      "\n",
      "\n",
      "#####Troubleshooting JOINs\n",
      "\n",
      "Make sure that your results are as expected, so consider what the observation is (the row), and rule check other columns:\n",
      "\n",
      "* Is your expected unique column unique?\n",
      "* Is there duplicate data elsewhere? \n",
      "\n",
      "A common check to see if your data is not unique is throwing a HAVING clause in your JOIN."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### HAVING\n",
      "Whereas WHERE is used to filter ungrouped data, HAVING is used to filter grouped (often called \"aggregated\" data) and works by filtering the data after the database engine has done performed some calculated on the aggregated data (ex. `count`, `sum`, `mean`, `variance`, etc.).\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    t1.c1,\n",
      "    t1.c2,\n",
      "    AVG(t2.c2)\n",
      "FROM t1 \n",
      "INNER JOIN t2 ON t1.c1 = t2.c2\n",
      "GROUP BY t1.c1, t1.c2\n",
      "HAVING AVG(t2.c2) > 10;\n",
      "```\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    poll_date,\n",
      "    AVG(obama_pct)\n",
      "FROM polls\n",
      "GROUP BY poll_date\n",
      "HAVING AVG(obama_pct) > 50;\n",
      "```\n",
      "\n",
      "***pandas***\n",
      "```python\n",
      "polls_group = polls.groupby('poll_date').obama_pct.filter(lambda x: x.mean() > 50)\n",
      "```\n",
      "***end_pandas***\n",
      "\n",
      "\n",
      "Note in this context HAVING allows us to filter on the computed column `AVG(t2.c2)` after the GROUP BY has run."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Connecting via Python\n",
      "We'll be using the `sqlite3` module from the Python standard library to connect to our database. Please follow Slack for instructions, however the syntax for connecting should be as follows:\n",
      "\n",
      "```python\n",
      "import sqlite3\n",
      "\n",
      "db = sqlite3.connect(\"<Absolute path to your DAT folder>/lahman2013.sqlite\")\n",
      "\n",
      "cur = db.cursor()\n",
      "\n",
      "cur.execute(\"SELECT COUNT(*) FROM Appearances;\")\n",
      "```\n",
      "\n",
      "\n",
      "The tables we'll need are below. If you need to look at columns, we can use this function:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your turn:\n",
      "\n",
      "#### Data Source\n",
      "\n",
      "In the class repo:\n",
      "\n",
      "`Data/lahman2013.sqlite`\n",
      "\n",
      "#### Dataset Documentation\n",
      "\n",
      "http://seanlahman.com/files/database/readme2014.txt\n",
      "\n",
      "#### Tables you'll need:\n",
      "\n",
      "```\n",
      "allstarfull\n",
      "fielding\n",
      "salaries\n",
      "schools\n",
      "schoolsplayers\n",
      "teams\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do your best to answer the following questions! They are sorted from simplest to most difficult in terms of SQL execution. If you'd like to practice your pandas syntax, submit both your SQL and pandas code (assuming tables were dataframes). CSV files of this dataset can also be found at: http://www.seanlahman.com/baseball-archive/statistics/.\n",
      "\n",
      "1. Show all playerids and salaries with a salary in the year 1985 above 500k.\n",
      "2. Show the team for each year that had a rank of 1.\n",
      "3. How many schools are in schoolstate of CT?\n",
      "4. How many schools are there in each state?\n",
      "5. What was the total spend on salaries by each team, each year?\n",
      "6. Find all of the salaries of shortstops  (fieldings, pos) for the year 2012.\n",
      "7. What is the first and last year played for each player?\n",
      "8. Who has played the most all star games? \n",
      "9. Which school has generated the most distinct players?\n",
      "10. Which school has generated the most expensive players? (expensive defined by their first year's salary).\n",
      "11. Show the 5 most expensive salaries for each team in the year 2014.\n",
      "12. Partition the average salaries by team and year, against year. Find players that were paid more than 1 standard deviation above the average salary for that team and year. Show a count by playerid.\n",
      "13. Calculate the win percentage. convert w and g into numerics (floats) to do so. (`w::numeric`, for example)\n",
      "14. Find the rank of each team each year against their actual total spend (defined as the sum of all player salaries) for that same year. \n",
      "   - Is there a correlation of spend to performance? \n",
      "   - What if we consider average player salary instead?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some tools and examples to help you get started:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import sqlite3 as sqlite\n",
      "\n",
      "db = sqlite.connect(\"<Absolute file path to your DAT folder>/Data/lahman2013.sqlite\")\n",
      "# Example: db = sqlite.connect(\"/Users/Aerlinger/Dropbox/GeneralAssembly/DAT_20_NYC/Data/lahman2013.sqlite\")\n",
      "\n",
      "##############################################################\n",
      "# Some useful functions:\n",
      "##############################################################\n",
      "\n",
      "# Simple method to convert a query string to a data frame\n",
      "def sql2df(query_string, limit=100, db=db):\n",
      "    cur = db.cursor()\n",
      "    cur.execute(query_string)\n",
      "\n",
      "    column_names = [description[0] for description in cur.description]\n",
      "    return pd.DataFrame(cur.fetchmany(limit), columns=column_names)\n",
      "\n",
      "# Print a query string to the console\n",
      "def preview_query(query_string, limit=10000000, db=db):\n",
      "    print sql2df(query_string, limit)\n",
      "\n",
      "# 1. Show all playerids and salaries with a salary in the year 1985 above 500k.\n",
      "preview_query('''\n",
      "\n",
      "SELECT * \n",
      "FROM Salaries \n",
      "WHERE yearID=1985 AND salary > 500000;\n",
      "\n",
      "''')\n",
      "\n",
      "\n",
      "# 2. Show the team for each year that had a rank of 1.\n",
      "preview_query('''\n",
      "\n",
      "SELECT * \n",
      "FROM TEAMS\n",
      "WHERE rank=1\n",
      "\n",
      "''')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OperationalError",
       "evalue": "unable to open database file",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-94e453a8885c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msqlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Absolute file path to your DAT folder>/Data/lahman2013.sqlite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example: db = sqlite.connect(\"/Users/Aerlinger/Dropbox/GeneralAssembly/DAT_20_NYC/Data/lahman2013.sqlite\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Additional Resources\n",
      "\n",
      "1. Read through Hadley Wickham's paper on [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf). While code samples are written in R it is an incredibly familiar concept and important on how we think about querying.\n",
      "2. Additional comparisons between pandas and sql syntax on the [pandas](http://pandas.pydata.org/pandas-docs/dev/comparison_with_sql.html) website.\n",
      "3. Some recommended GUIs for interacting with postgres (and other databases):\n",
      "    1. [Postico](https://eggerapps.at/postico/) is the new version of PG Commander built for Mac OS X.\n",
      "    2. [RazorSQL](http://razorsql.com/) is cross platform and cross DB, but also expensive for the product (not free)\n",
      "4. If you're looking for something in python between pandas and psycopg2 in terms of complexity, [dataset](http://dataset.readthedocs.org/en/latest/api.html) is a great python module to learn, particularly for moving data around.\n",
      "5. Additional SQL Help:\n",
      "    1. [SQL Bootcamp](https://github.com/brandonmburroughs/sql_bootcamp) from GA (MySQL)\n",
      "    2. [SQLZOO](http://sqlzoo.net/wiki/SELECT) can switch between engines\n",
      "    3. [w3schools](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all) has a practice database.\n",
      "    4. [SQLSchool](http://sqlschool.modeanalytics.com/)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}