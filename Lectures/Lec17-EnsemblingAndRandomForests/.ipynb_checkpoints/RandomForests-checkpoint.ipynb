{
 "metadata": {
  "name": "",
  "signature": "sha256:bdb4bdbdd7abdf54ae0a6ffcbfb8b0856300bf06cd77fd168210e8f7c34c9db9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ensemble Learning with Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Learning Objectives**\n",
      "1. Ensemble Techniques\n",
      "1. Problems In Classification\n",
      "1. Bagging\n",
      "1. Boosting\n",
      "1. Implementing Random Forests\n",
      "\n",
      "**Labs:**\n",
      "1. Random Forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ensemble methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ensemble methods are learning algorithms that construct a set of classfiers and then classify new data points by taking a (weighted) vote of their predictions. They are methods of improving classification accuracy by aggregating predictions over several base classifiers. Ensembles are often much more accurate than the base classifiers that compose them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order for an ensemble classifier to outperform a single base classifier (bc), two conditions must be met. The base classifiers must be:\n",
      "1. *accurate*: they must outperform random guessing ~ _low bias_\n",
      "1. *diverse*: their misclassifications must occur on different training examples ~ _uncorrelated_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Combining results from many different classifiers also results in a lensemble classifier with lower overall variance than any single classifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/ensemble.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Problems In Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In any supervised learning task, our goal is to make predictions of the\n",
      "true classification function `f` by learning the classifier `h`.\n",
      "There are three main problems that can prevent this:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Statistical problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the amount of training data available is small, the base classifier\n",
      "will have difficulty converging to $h$ . An ensemble classifier can mitigate this problem by \u201caveraging out\u201d base classifier predictions to improve convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/statistical.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true function $f$ is best approximated as an average of the base classifiers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Computational problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even with sufficient training data, it may still be computationally\n",
      "difficult to find the best classifier h. For example, if our base classifier is a decision tree, an exhaustive search of the hypothesis space of all possible classifiers is extremely complex (NP-complete)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that this is why we used a heuristic algorithm (greedy search)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An ensemble composed of several BC\u2019s with different starting points can provide a better approximation to f than any individual BC."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/computational.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true function $f$ is often best approximated by using several starting points to explore the hypothesis space."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Representational problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes $f$ cannot be expressed in terms of our hypothesis at all. To illustrate this, suppose we use a decision tree as our base classifier. A decision tree works by forming a rectilinear partition of the feature space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/rectilinear.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But what if $f$ is a diagonal line? Then it cannot be represented by finitely many rectilinear segments, and therefore the true decision boundary cannot be obtained by a decision tree classifier. However, it may be still be possible to approximate $f$ or even to expand the space of representable functions using ensemble methods."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An ensemble of decision trees can approximate a diagonal decision boundary."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/approximation.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/representational.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ensemble classifiers can be effective even if the true decision boundary lies outside the hypothesis space."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Creating an Ensemble Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/ensemblecreation.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several ways to generate base classifiers\n",
      "* manipulating the training set\n",
      "* manipulating the learning algorithm itself\n",
      "* manipulating the output labels"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Manipulating the training set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These technique works especially well for unstable learning algorithms|algorithms whose output classifier undergoes major changes in response to small changes in the training data. Decision-tree, neural network, and rule learning algorithms are all unstable. Linear regression, nearest neighbor, and linear threshold algorithms are generally very stable."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bagging (bootstrap aggregating) is a method that involves manipulating the training set by *resampling*. We learn $k$ base classifiers on $k$ different samples of training data. These samples are independently created by resampling the training data using uniform weights (eg, a *uniform sampling distribution*)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Each training sample is the same size as the original training set.\n",
      "* Resampling means that some training records may appear in a sample more than once, or even not at all."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final prediction is made by taking a majority vote across bc\u2019s."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bagging reduces the variance in our generalization error by aggregating multiple base classifiers together (provided they satisfy our earlier requirements)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the base classifier is stable, then the ensemble error is primarily due to bc bias, and bagging may not be effective. Since each sample of training data is equally likely, bagging is not very susceptible to overfitting with noisy data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/bagging_ozone.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_But taking the average of 100 smoothers, each fitted to a subset of the original data set, we arrive at one bagged predictor (red line). Clearly, the mean is more stable and there is less overfit._"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "sklearn.ensemble.BaggingClassifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When random subsets of the dataset are drawn as _random subsets of the samples_, then this algorithm is known as *Pasting*. If samples are _drawn with replacement_, then the method is known as *Bagging*. When random subsets of the dataset are drawn as _random subsets of the features_, then the method is known as *Random Subspaces*. Finally, when base estimators are built on _subsets of both samples and features_, then the method is known as *Random Patches*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "ensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
      "                             bootstrap=True,\n",
      "                             bootstrap_features=False)\n",
      "\n",
      "# ensemble.fit(X_train, y_train)\n",
      "# BaggingClassifier?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/ensemble_boosting.jpg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Boosting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Boosting is an iterative procedure that adaptively changes the sampling distribution of training records at each iteration. The first iteration uses uniform weights (like bagging). In subsequent iterations, the weights are adjusted to emphasize records that were misclassified in previous iterations. The final prediction is constructed by a weighted vote (where the weights for a bc depends on its training error)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bc\u2019s focus more and more closely on records that are difficult to classify as the sequence of iterations progresses. Thus the bc\u2019s are faced with progressively more difficult learning problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Like in bagging, sampling is done with replacement, and as a result some records may not appear in a given training sample. These omitted records will likely be misclassified, and given greater weight in subsequent iterations once the sampling distribution is updated. So even if a record is left out at one stage, it will be emphasized later."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Updating the sampling distribution and forming an ensemble prediction leads to a nonlinear combination of the base classifiers. By explicitly trying to optimize the weighted ensemble vote, boosting attacks the representation problem head-on."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A random forest is an ensemble of decision trees where each base classifier is grown using a random effect. One way to do this is to randomly choose one of the top $k$ features to split each node. For a small number of features, we can also create linear combinations of features and select splits from the enhanced feature set (Forest-RC). Or, we can select splitting features completely at random (Forest-RI)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random forests are about as accurate as AdaBoost, more robust to noise, and can also have better runtime than other ensemble methods (since the feature space is reduced in some cases)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
      "     random_state=0)\n",
      "\n",
      "## DecisionTreeClassifier\n",
      "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1,\n",
      "       random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()\n",
      "\n",
      "## RandomForestClassifier\n",
      "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
      "     min_samples_split=1, random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()\n",
      "\n",
      "## ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
      "    min_samples_split=1, random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.979408793821\n",
        "0.999607843137"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.999898989899"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main parameters to adjust when using these methods is `n_estimators` and `max_features`. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are `max_features=n_features` for regression problems, and `max_features=sqrt(n_features)` for classification tasks (where `n_features` is the number of features in the data). The best results are also usually reached when setting `max_depth=None` in combination with `min_samples_split=1` (i.e., when fully developing the trees)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example Use Cases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Wisdom of Crowds\n",
      "* IBM\u2019s WATSON\n",
      "* Nate Silver\u2019s election models\n",
      "* Kaggle competitions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementing Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot compares the decision surfaces learned by a decision tree classifier (first column), by a random forest classifier (second column), by an extra-trees classifier (third column) and by an AdaBoost classifier (fourth column)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn import clone\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# note: these imports are incorrect in the example online!\n",
      "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
      "from sklearn.ensemble.forest import (RandomForestClassifier,\n",
      "                                        ExtraTreesClassifier)\n",
      "\n",
      "from sklearn.externals.six.moves import xrange\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Parameters\n",
      "n_classes = 3\n",
      "n_estimators = 30\n",
      "plot_colors = \"bry\"\n",
      "plot_step = 0.02\n",
      "\n",
      "# Load data\n",
      "iris = load_iris()\n",
      "\n",
      "plot_idx = 1\n",
      "\n",
      "for pair in ([0, 1], [0, 2], [2, 3]):\n",
      "    for model in (DecisionTreeClassifier(),\n",
      "                  RandomForestClassifier(n_estimators=n_estimators),\n",
      "                  ExtraTreesClassifier(n_estimators=n_estimators),\n",
      "                  AdaBoostClassifier(DecisionTreeClassifier(),\n",
      "                                     n_estimators=n_estimators)):\n",
      "        # We only take the two corresponding features\n",
      "        X = iris.data[:, pair]\n",
      "        y = iris.target\n",
      "\n",
      "        # Shuffle\n",
      "        idx = np.arange(X.shape[0])\n",
      "        np.random.seed(13)\n",
      "        np.random.shuffle(idx)\n",
      "        X = X[idx]\n",
      "        y = y[idx]\n",
      "\n",
      "        # Standardize\n",
      "        mean = X.mean(axis=0)\n",
      "        std = X.std(axis=0)\n",
      "        X = (X - mean) / std\n",
      "\n",
      "        # Train\n",
      "        clf = model.fit(X, y)\n",
      "\n",
      "        # Get accuracy scores\n",
      "        scores = clf.score(X, y)\n",
      "        # Create a title for each column and the console by using str() and slicing away useless parts of the string\n",
      "        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
      "        model_details = model_title\n",
      "        if hasattr(model, \"estimators_\"):\n",
      "            model_details += \" with {} estimators\".format(len(model.estimators_))\n",
      "        print model_details + \" with features\", pair, \"has a score of\", scores\n",
      "\n",
      "        # Plot the decision boundary\n",
      "        pl.subplot(3, 4, plot_idx)\n",
      "\n",
      "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
      "                             np.arange(y_min, y_max, plot_step))\n",
      "\n",
      "        if isinstance(model, DecisionTreeClassifier):\n",
      "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "            Z = Z.reshape(xx.shape)\n",
      "            cs = pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "        else:\n",
      "            for tree in model.estimators_:\n",
      "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "                Z = Z.reshape(xx.shape)\n",
      "                cs = pl.contourf(xx, yy, Z, alpha=0.1, cmap=pl.cm.Paired)\n",
      "\n",
      "        pl.axis(\"tight\")\n",
      "\n",
      "        # Plot the training points\n",
      "        for i, c in zip(xrange(n_classes), plot_colors):\n",
      "            idx = np.where(y == i)\n",
      "            pl.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n",
      "                       cmap=pl.cm.Paired)\n",
      "\n",
      "        pl.axis(\"tight\")\n",
      "\n",
      "        plot_idx += 1\n",
      "\n",
      "pl.suptitle(\"Decision surfaces of DecisionTreeClassifier, \"\n",
      "            \"RandomForestClassifier,\\nExtraTreesClassifier\"\n",
      "            \" and AdaBoostClassifier\")\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTree with features [0, 1] has a score of 0.926666666667\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "AdaBoost with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "DecisionTree with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "AdaBoost with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "DecisionTree with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "AdaBoost with 30 estimators with features"
       ]
      }
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Application to other data sets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we have a dataset of chapters from books and plays by specific authors, and their usages of stop words. Let's see how accurately a random forest can predict the author based on stop word usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "from pandas import read_csv\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "\n",
      "authorship = read_csv('../../data/authorship.csv')\n",
      "print authorship.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "authors = list(set(authorship.Author.values))\n",
      "print authors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(authors)\n",
      "authorship['Author_num'] = le.transform(authorship['Author'])\n",
      "print authorship['Author_num']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a random variable (random forests work best with a random variable)\n",
      "authorship['random'] = [random.random() for i in range(841)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are some of the stop words we're looking at?\n",
      "features = list(authorship.columns)\n",
      "features.remove('Author')\n",
      "features.remove('Author_num')\n",
      "features.remove('BookID')\n",
      "print features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a test and training set\n",
      "x_train, x_test, y_train, y_test = train_test_split(authorship[features], authorship.Author_num.values, test_size=0.4, random_state=123)\n",
      "x, y = authorship[features], authorship.Author_num.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit Model\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "etclf = ExtraTreesClassifier(n_estimators=20)\n",
      "etclf.fit(x_train, y_train)\n",
      "\n",
      "scores = cross_val_score(etclf, x, y)\n",
      "print scores.mean()\n",
      "\n",
      "# Print Confusion Matrix\n",
      "print metrics.confusion_matrix(etclf.predict(x_test), y_test)\n",
      "print authors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. With the authorship data, determine how changing the parameters in the random forest model changes the performance of the model.\n",
      "\n",
      "2. Also with the authorship data, feel free to go back to the base random forest classifer included in sklearn, or see how using adaboost does on guess work.\n",
      "\n",
      "3. Try [timing](http://nbviewer.ipython.org/gist/anonymous/5978528) adaboost in comparison to randomforests to see how performance changes.\n",
      "\n",
      "4. Consider building your own bagging algorithm (or get crazy and see if you can write up a simple boosting one) on your own. While this is relatively efficient in python, R users tend to complain a lot about how slow ensemble methods are (from the base packages). Building a strong understanding of these approaches can really move you along in the world of machine learning!\n",
      "\n",
      "5. How can ensemble methods be distributed across a cluster of servers? Can they be?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [Random Forests in Python](http://blog.yhathq.com/posts/random-forests-in-python.html)\n",
      "- [Random Forests for Kaggle](http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-random-forests)\n",
      "- [Random Forests and Performance Metrics](http://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Academic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Original paper on Random Forests](http://oz.berkeley.edu/~breiman/randomforest2001.pdf)\n",
      "* [Ensemble Methods in Machine Learning](http://www.ensemblemethods.com/documents/EnsembleMethodsInMachineLearning.pdf)\n",
      "* [Explaining Bagging](ftp://stat.ethz.ch/Research-Reports/92.pdf)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Modules"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [sklearn.ensemble.BaggingClassifier](http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}